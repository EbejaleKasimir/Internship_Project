{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "\n",
    "def process_review_count(text):\n",
    "    text = text.strip().replace(',', '')\n",
    "    if 'K+' in text:\n",
    "        return str(int(float(text.replace('(', '').replace(')', '').replace('K+', '').strip()) * 1000))\n",
    "    return text\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    # options.add_argument('--headless')  # Run browser in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    try:\n",
    "        driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Failed to install Edge Chromium driver.\")\n",
    "    return driver\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    seen_products = set()  # Initialize the set here\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        for page in range(1, 10):\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                # Increase the wait time if needed\n",
    "                WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-asin]\")))\n",
    "            except TimeoutException:\n",
    "                print(f\"Timed out waiting for elements on page {page} of category {category}.\")\n",
    "                continue\n",
    "\n",
    "            # Use random sleep to mimic human behavior\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "            # Now parse the page with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#\n",
    "            for product in soup.find_all('div', attrs={\"data-asin\": True}):\n",
    "                product_dict = {}\n",
    "\n",
    "                # Extract Product_ID (ASIN) directly from the 'data-asin' attribute\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-text-normal')\n",
    "                if item_name:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "#\n",
    "                # Item price\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price:\n",
    "                    product_price = product_price.text.strip().replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "                    product_dict['price'] = product_price\n",
    "\n",
    "                # Ratings and review responders\n",
    "                rating_spans = product.find_all('span', attrs={\"aria-label\": True})\n",
    "                for rating_span in rating_spans:\n",
    "                    aria_label_value = rating_span.attrs[\"aria-label\"]\n",
    "                    if \"stars\" in aria_label_value:\n",
    "                        product_dict['ratings'] = aria_label_value.split(\" \")[0]\n",
    "                    else:\n",
    "                        if 'K+' in aria_label_value:\n",
    "                            product_dict['review_responders'] = aria_label_value\n",
    "                        else:\n",
    "                            try:\n",
    "                                int_value = int(aria_label_value)\n",
    "                                product_dict['review_responders'] = aria_label_value\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "\n",
    "                # Updated Item reviews extraction\n",
    "                item_reviews = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if item_reviews:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = process_review_count(reviews_text)\n",
    "                    product_dict['reviews'] = reviews_count\n",
    "\n",
    "                # # Extract item URL\n",
    "                # item_url_tag = product.find('a', class_='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "                # if item_url_tag:\n",
    "                #     item_url = item_url_tag.get('href')\n",
    "                #     product_dict['url'] = \"https://www.amazon.com\" + item_url\n",
    "                # else:\n",
    "                #     product_dict['url'] = None\n",
    "\n",
    "                # Extract item URL\n",
    "                item_url_tag = product.find('a', class_='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "                if item_url_tag:\n",
    "                    item_url = item_url_tag.get('href')\n",
    "                    \n",
    "                    # Check if it is a relative URL\n",
    "                    if item_url.startswith('/'):\n",
    "                        product_dict['url'] = \"https://www.amazon.com\" + item_url.split('/ref')[0] + \"/product-reviews/\" + product_dict['Product_ID'] + \"/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "                    else:\n",
    "                        product_dict['url'] = item_url\n",
    "                else:\n",
    "                    product_dict['url'] = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Ensure product dictionary contains necessary details\n",
    "                if 'Product_ID' in product_dict and product_dict['Product_ID']:\n",
    "                    # Create a unique identifier for the product\n",
    "                    identifier = product_dict['Product_ID']\n",
    "\n",
    "                    if identifier not in seen_products:\n",
    "                        seen_products.add(identifier)\n",
    "                        products.append(product_dict)\n",
    "\n",
    "            all_products.extend(products)\n",
    "\n",
    "    driver.quit()\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    categories = {\n",
    "        'Smartphones': 'https://www.amazon.com/s?k=smartphone&ref=nb_sb_noss',\n",
    "        'Laptops': 'https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss',\n",
    "        'video_games': 'https://www.amazon.com/s?k=video_games&ref=nb_sb_noss',\n",
    "        'Dresses':'https://www.amazon.com/s?k=Dresses&ref=nb_sb_noss',\n",
    "        'Shoes':'https://www.amazon.com/s?k=Shoes&ref=nb_sb_noss',\n",
    "        'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&ref=nb_sb_noss',\n",
    "    }\n",
    "\n",
    "    amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "    # Save the JSON data to a file\n",
    "    with open('amazon_data_cat.json', 'w') as file:\n",
    "        json.dump(amazon_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_cat.json')\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['Product_ID'], keep='first')\n",
    "\n",
    "# Fill NaN values in 'reviews' with '0'\n",
    "df['reviews'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'ratings' with '0'\n",
    "df['ratings'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'price' with '0'\n",
    "df['price'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'reviews' with 0\n",
    "# df['reviews'] = df['reviews'].str.replace('(', '').str.replace(')', '')\n",
    "df['reviews'].fillna(0, inplace=True)\n",
    "# Check the data type of the reviews column\n",
    "if pd.api.types.is_string_dtype(df['reviews']):\n",
    "    df['reviews'] = df['reviews'].str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "# Convert reviews to integer\n",
    "df['reviews'] = df['reviews'].astype(int)\n",
    "\n",
    "# Set negative reviews to 0\n",
    "df.loc[df['reviews'] < 0, 'reviews'] = 0\n",
    "\n",
    "# Fill NaN values in 'url' with 'Unknown'\n",
    "df['url'].fillna('Unknown', inplace=True)\n",
    "\n",
    "\n",
    "# Function to extract the second occurrence of the URL\n",
    "def extract_second_url(url):\n",
    "    prefix = \"https://www.amazon.comhttps://\"\n",
    "    if url.startswith(prefix):\n",
    "        matches = re.findall(r'https://www\\.amazon\\.com/', url[len(prefix):])\n",
    "        if len(matches) >= 1:\n",
    "            second_occurrence_index = url.rfind(matches[0])\n",
    "            return url[second_occurrence_index:]\n",
    "    return url\n",
    "\n",
    "# Apply the function to the 'url' column\n",
    "df['url'] = df['url'].apply(extract_second_url)\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['product', 'price','ratings', 'reviews', 'category'], keep='first')\n",
    "\n",
    "# Replace empty product names with NaN and drop those rows\n",
    "df['product'].replace('', pd.NA, inplace=True)\n",
    "df.dropna(subset=['product'], inplace=True)\n",
    "\n",
    "# Drop the 'review_responders' column\n",
    "df.drop('review_responders', axis=1, inplace=True)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table in the PostgreSQL database\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_data;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data (\n",
    "    product_id TEXT NOT NULL,\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NOT NULL,\n",
    "    ratings NUMERIC NOT NULL,\n",
    "    reviews INTEGER NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    url TEXT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "def clean_format_data(row):\n",
    "    # Convert the ratings value to a float\n",
    "    ratings = float(row['ratings'])\n",
    "    \n",
    "    # Convert the product name to a string and then adapt for SQL insertion\n",
    "    product = psycopg2.extensions.adapt(str(row['product']).encode('utf-8')).getquoted().decode('utf-8')[1:-1]\n",
    "    \n",
    "    # Convert price to float, if not possible set to 0\n",
    "    try:\n",
    "        price = float(row['price'])\n",
    "    except ValueError:\n",
    "        price = 0\n",
    "\n",
    "    category = psycopg2.extensions.adapt(row['category']).getquoted().decode('utf-8')[1:-1]\n",
    "    url = row['url']\n",
    "    product_id = row['Product_ID']\n",
    "    reviews = row['reviews']  # Already cleaned and converted to int\n",
    "    return product_id, product, price, ratings, reviews, category, url\n",
    "\n",
    "\n",
    "# Insert the data from the pandas DataFrame into the PostgreSQL table\n",
    "for index, row in df.iterrows():\n",
    "    product_id, product, price, ratings, reviews, category, url = clean_format_data(row)\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO amazon_data (product_id, product, price, ratings, reviews, category, url) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cur.execute(insert_query, (product_id, product, price, ratings, reviews, category, url))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('amazon_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>product</th>\n",
       "      <th>price</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>review_responders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10173</th>\n",
       "      <td>B0B6Z4XVVG</td>\n",
       "      <td>HOTOUCH Waffle Button Down Shirt Women Casual ...</td>\n",
       "      <td>32.99</td>\n",
       "      <td>4.1</td>\n",
       "      <td>545.0</td>\n",
       "      <td>https://www.amazon.com/sspa/click?ie=UTF8&amp;spc=...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10174</th>\n",
       "      <td>B07KFB347M</td>\n",
       "      <td>Alise YJ8000-B Clothing Multiple Hook Wardrobe...</td>\n",
       "      <td>11.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>https://www.amazon.com/sspa/click?ie=UTF8&amp;spc=...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10175</th>\n",
       "      <td>B0BNVGCY42</td>\n",
       "      <td>SORON 43\" Garment Bags, 7 Packs Garment Bags f...</td>\n",
       "      <td>26.99</td>\n",
       "      <td>4.6</td>\n",
       "      <td>212.0</td>\n",
       "      <td>https://www.amazon.com/sspa/click?ie=UTF8&amp;spc=...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10176</th>\n",
       "      <td>B0BF8S8KX6</td>\n",
       "      <td>Topwon Bear Ear Women Man Winter Ski Mask Knit...</td>\n",
       "      <td>14.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.amazon.com/sspa/click?ie=UTF8&amp;spc=...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10177</th>\n",
       "      <td>B01N9PB4CX</td>\n",
       "      <td>DreamLily Lace Cat Ears Hair Band Fancy Dress ...</td>\n",
       "      <td>9.99</td>\n",
       "      <td>4.5</td>\n",
       "      <td>922.0</td>\n",
       "      <td>https://www.amazon.com/sspa/click?ie=UTF8&amp;spc=...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>922.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Product_ID                                            product  price  \\\n",
       "10173  B0B6Z4XVVG  HOTOUCH Waffle Button Down Shirt Women Casual ...  32.99   \n",
       "10174  B07KFB347M  Alise YJ8000-B Clothing Multiple Hook Wardrobe...  11.99   \n",
       "10175  B0BNVGCY42  SORON 43\" Garment Bags, 7 Packs Garment Bags f...  26.99   \n",
       "10176  B0BF8S8KX6  Topwon Bear Ear Women Man Winter Ski Mask Knit...  14.99   \n",
       "10177  B01N9PB4CX  DreamLily Lace Cat Ears Hair Band Fancy Dress ...   9.99   \n",
       "\n",
       "       ratings  reviews                                                url  \\\n",
       "10173      4.1    545.0  https://www.amazon.com/sspa/click?ie=UTF8&spc=...   \n",
       "10174      4.3     52.0  https://www.amazon.com/sspa/click?ie=UTF8&spc=...   \n",
       "10175      4.6    212.0  https://www.amazon.com/sspa/click?ie=UTF8&spc=...   \n",
       "10176      4.2      4.0  https://www.amazon.com/sspa/click?ie=UTF8&spc=...   \n",
       "10177      4.5    922.0  https://www.amazon.com/sspa/click?ie=UTF8&spc=...   \n",
       "\n",
       "          category  review_responders  \n",
       "10173  Accessories              545.0  \n",
       "10174  Accessories               52.0  \n",
       "10175  Accessories              212.0  \n",
       "10176  Accessories                4.0  \n",
       "10177  Accessories              922.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_cat.json')\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['Product_ID'], keep='first')\n",
    "\n",
    "# Fill NaN values in 'reviews' with 0\n",
    "# df['reviews'] = df['reviews'].str.replace('(', '').str.replace(')', '')\n",
    "df['reviews'].fillna(0, inplace=True)\n",
    "# Check the data type of the reviews column\n",
    "if pd.api.types.is_string_dtype(df['reviews']):\n",
    "    df['reviews'] = df['reviews'].str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 lines per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_cat.json')\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['Product_ID'], keep='first')\n",
    "\n",
    "# Fill NaN values in 'reviews' with '0'\n",
    "df['reviews'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'ratings' with '0'\n",
    "df['ratings'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'price' with '0'\n",
    "df['price'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill NaN values in 'reviews' with 0\n",
    "\n",
    "df['reviews'].fillna(0, inplace=True)\n",
    "# Check the data type of the reviews column\n",
    "if pd.api.types.is_string_dtype(df['reviews']):\n",
    "    df['reviews'] = df['reviews'].str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "# Convert reviews to integer\n",
    "df['reviews'] = df['reviews'].astype(int)\n",
    "\n",
    "# Set negative reviews to 0\n",
    "df.loc[df['reviews'] < 0, 'reviews'] = 0\n",
    "\n",
    "# Fill NaN values in 'url' with 'Unknown'\n",
    "df['url'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Function to extract the second occurrence of the URL\n",
    "def extract_second_url(url):\n",
    "    prefix = \"https://www.amazon.comhttps://\"\n",
    "    if url.startswith(prefix):\n",
    "        matches = re.findall(r'https://www\\.amazon\\.com/', url[len(prefix):])\n",
    "        if len(matches) >= 1:\n",
    "            second_occurrence_index = url.rfind(matches[0])\n",
    "            return url[second_occurrence_index:]\n",
    "    return url\n",
    "\n",
    "# Apply the function to the 'url' column\n",
    "df['url'] = df['url'].apply(extract_second_url)\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['product', 'price','ratings', 'reviews', 'category'], keep='first')\n",
    "\n",
    "# Replace empty product names with NaN and drop those rows\n",
    "df['product'].replace('', pd.NA, inplace=True)\n",
    "df.dropna(subset=['product'], inplace=True)\n",
    "\n",
    "# Drop the 'review_responders' column\n",
    "df.drop('review_responders', axis=1, inplace=True)\n",
    "\n",
    "# Group by 'category' and select the first 100 rows of each group\n",
    "df_grouped = df.groupby('category')\n",
    "df_sampled = df_grouped.apply(lambda x: x.sample(min(len(x), 100))).reset_index(drop=True)\n",
    "df = df_sampled\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table in the PostgreSQL database\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_data_100;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data_100 (\n",
    "    product_id TEXT NOT NULL,\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NOT NULL,\n",
    "    ratings NUMERIC NOT NULL,\n",
    "    reviews INTEGER NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    url TEXT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "def clean_format_data(row):\n",
    "    # Convert the ratings value to a float\n",
    "    ratings = float(row['ratings'])\n",
    "    \n",
    "    # Convert the product name to a string and then adapt for SQL insertion\n",
    "    product = psycopg2.extensions.adapt(str(row['product']).encode('utf-8')).getquoted().decode('utf-8')[1:-1]\n",
    "    \n",
    "    # Convert price to float, if not possible set to 0\n",
    "    try:\n",
    "        price = float(row['price'])\n",
    "    except ValueError:\n",
    "        price = 0\n",
    "\n",
    "    category = psycopg2.extensions.adapt(row['category']).getquoted().decode('utf-8')[1:-1]\n",
    "    url = row['url']\n",
    "    product_id = row['Product_ID']\n",
    "    reviews = row['reviews']  # Already cleaned and converted to int\n",
    "    return product_id, product, price, ratings, reviews, category, url\n",
    "\n",
    "\n",
    "# Insert the data from the pandas DataFrame into the PostgreSQL table\n",
    "for index, row in df.iterrows():\n",
    "    product_id, product, price, ratings, reviews, category, url = clean_format_data(row)\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO amazon_data_100 (product_id, product, price, ratings, reviews, category, url) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cur.execute(insert_query, (product_id, product, price, ratings, reviews, category, url))\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('amazon_data_100.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
