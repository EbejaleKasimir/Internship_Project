{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:WDM:====== WebDriver manager ======\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:There is no [win64] edgedriver \"117.0.2045.43\" for browser edge \"117.0.2045\" in cache\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:About to download new driver from https://msedgedriver.azureedge.net/117.0.2045.43/edgedriver_win64.zip\n",
      "INFO:WDM:Driver downloading response is 200\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Driver has been saved in cache [C:\\Users\\Kasim\\.wdm\\drivers\\edgedriver\\win64\\117.0.2045.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 82 people found this helpful\n",
      "Helpful Text: 82 people found this helpful\n",
      "Helpful Text: 82 people found this helpful\n",
      "Helpful Text: 82 people found this helpful\n",
      "Helpful Text: 82 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n",
      "TimeoutException: Could not find reviews for https://www.amazon.com/product-reviews/B0C37QXBH3/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 5 people found this helpful\n",
      "Helpful Text: 56 people found this helpful\n",
      "Helpful Text: 56 people found this helpful\n",
      "Helpful Text: 56 people found this helpful\n",
      "Helpful Text: 56 people found this helpful\n",
      "Helpful Text: 56 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 663 people found this helpful\n",
      "Helpful Text: 663 people found this helpful\n",
      "Helpful Text: 663 people found this helpful\n",
      "Helpful Text: 663 people found this helpful\n",
      "Helpful Text: 663 people found this helpful\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Helpful Text: 6 people found this helpful\n",
      "Helpful Text: 6 people found this helpful\n",
      "Helpful Text: 6 people found this helpful\n",
      "Helpful Text: 6 people found this helpful\n",
      "Helpful Text: 6 people found this helpful\n",
      "Helpful Text: 69 people found this helpful\n",
      "Helpful Text: 69 people found this helpful\n",
      "Helpful Text: 69 people found this helpful\n",
      "Helpful Text: 69 people found this helpful\n",
      "Helpful Text: 69 people found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: 11 people found this helpful\n",
      "Helpful Text: 11 people found this helpful\n",
      "Helpful Text: 11 people found this helpful\n",
      "Helpful Text: 11 people found this helpful\n",
      "Helpful Text: 11 people found this helpful\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Helpful Text: 10 people found this helpful\n",
      "Helpful Text: 10 people found this helpful\n",
      "Helpful Text: 10 people found this helpful\n",
      "Helpful Text: 10 people found this helpful\n",
      "Helpful Text: 10 people found this helpful\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "TimeoutException: Could not find reviews for https://www.amazon.com/product-reviews/B0CGVH5HWB/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Helpful Text: 107 people found this helpful\n",
      "Helpful Text: 107 people found this helpful\n",
      "Helpful Text: 107 people found this helpful\n",
      "Helpful Text: 107 people found this helpful\n",
      "Helpful Text: 107 people found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n",
      "Tag not found\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import traceback\n",
    "import logging\n",
    "import re  # Make sure to include this import\n",
    "from word2number import w2n\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_review_count(text):\n",
    "    text = text.strip().replace(',', '')\n",
    "    if 'K+' in text:\n",
    "        return str(int(float(text.replace('(', '').replace(')', '').replace('K+', '').strip()) * 1000))\n",
    "    return text\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    try:\n",
    "        driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Failed to install Edge Chromium driver.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "def scrape_extra_parameters(url: str, driver: webdriver.Edge) -> dict:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"TimeoutException: Could not find reviews for {url}\")\n",
    "            return {}\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract the general reviews\n",
    "        reviews_tags = soup.find_all('div', attrs={'data-hook': 'review'})\n",
    "\n",
    "        result = {}\n",
    "        for i, review_tag in enumerate(reviews_tags[:5]):\n",
    "            result[f'Customer_{i + 1}_ID'] = review_tag.attrs.get('id', 'None')\n",
    "            \n",
    "            # Extract the Star Rating\n",
    "            star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating\"] span.a-icon-alt')\n",
    "            star_rating = float(star_rating_tag.text.split()[0]) if star_rating_tag else 0.0\n",
    "            result[f'Customer_{i+1}_Star_Rating'] = star_rating\n",
    "            \n",
    "            # Extract the Comment Title\n",
    "            comment_title_tag = review_tag.select_one('a[data-hook=\"review-title\"]')\n",
    "            # Inside the for loop, after extracting the comment title:\n",
    "            if comment_title_tag:\n",
    "                actual_comment_title = comment_title_tag.text.strip()\n",
    "            else:\n",
    "                # Handle alternate structure\n",
    "                comment_title_tag = review_tag.select_one('span.cr-original-review-content')\n",
    "                actual_comment_title = comment_title_tag.text.strip() if comment_title_tag else 'NaN'\n",
    "\n",
    "            # Remove the pattern \"k out of 5 stars\\n\" from the comment\n",
    "            actual_comment_title = re.sub(r'\\d+(\\.\\d+)? out of 5 stars\\n', '', actual_comment_title)\n",
    "\n",
    "            result[f'Customer_{i+1}_Comment'] = actual_comment_title\n",
    "\n",
    "            # Extract the Number of people who found the review helpful\n",
    "            helpful_vote_tag = review_tag.select_one('span[data-hook=\"helpful-vote-statement\"]')\n",
    "            helpful_count = w2n.word_to_num(helpful_vote_tag.text.split()[0]) if helpful_vote_tag else 0\n",
    "            result[f'Customer_{i+1}_buying_influence'] = helpful_count\n",
    "        \n",
    "            # Extract all elements matching the criteria\n",
    "            critical_review_tags = soup.select('div[id^=\"viewpoint-\"]')\n",
    "            # critical_review_tags = soup.select('div.a-column.a-span6.view-point-review.critical-review.a-span-last')\n",
    "            if len(critical_review_tags) > 1:\n",
    "                # If there is more than one matching element, select the second one\n",
    "                critical_review_tag = critical_review_tags[1]\n",
    "                critical_review_tag_pack = critical_review_tag.get('id', 'None').replace('viewpoint-', '')\n",
    "                result['Customer_id_Critical_Review'] = critical_review_tag_pack\n",
    "                \n",
    "                # Extract Customer_Name\n",
    "                customer_name_tags = soup.select('span.a-profile-name')\n",
    "                result['Customer_Name'] = customer_name_tags[1].text if len(customer_name_tags) > 1 else 'None'\n",
    "                    \n",
    "                # Extract Customer_Review_Comment                \n",
    "                review_comment_tag = critical_review_tag.find('div', class_='a-row a-spacing-top-mini')\n",
    "                result['Customer_Review_Comment'] = review_comment_tag.text.strip() if review_comment_tag else 'None'\n",
    "                \n",
    "                # Extract Customer_Review_Title\n",
    "                review_title_tag = critical_review_tag.select_one('span[data-hook=\"review-title\"]')\n",
    "                result['Customer_Review_Title'] = review_title_tag.text if review_title_tag else 'None'\n",
    "\n",
    "                # Extract the post time\n",
    "                critical_review_tags_date = critical_review_tag.select('div.a-expander-content.a-expander-partial-collapse-content span.a-size-base.a-color-secondary.review-date')\n",
    "                if critical_review_tags_date:\n",
    "                    post_time_text = critical_review_tags_date[0].text.strip()\n",
    "                    match = re.search(r'on (.+)$', post_time_text)\n",
    "                    if match:\n",
    "                        date_string = match.group(1)\n",
    "                        try:\n",
    "                            post_date = datetime.strptime(date_string, '%B %d, %Y')\n",
    "                            # Convert the datetime object to a string in ISO format\n",
    "                            result['Post_Date'] = post_date.isoformat()                            \n",
    "                        except ValueError as ve:\n",
    "                            print(f\"Error parsing date string {date_string}: {ve}\")\n",
    "                            result['Post_Date'] = '-'\n",
    "                    else:\n",
    "                        print(\"Date not found in text:\", post_time_text)\n",
    "                        result['Post_Date'] = '-'\n",
    "                else:\n",
    "                    print(\"Date tag not found\")\n",
    "                    result['Post_Date'] = None\n",
    "\n",
    "                # Use soup.select() to find all matching elements\n",
    "                critical_review_tags_ = soup.select('div.a-column.a-span6.view-point-review.critical-review.a-span-last div.a-row.a-spacing-top-small span.a-size-small.a-color-tertiary span.review-votes')\n",
    "\n",
    "                # Check if any elements were found\n",
    "                if critical_review_tags_:\n",
    "                    # Take the first found element (if there are multiple) and directly extract the text\n",
    "                    helpful_text = critical_review_tags_[0].text.strip()\n",
    "                    print(\"Helpful Text:\", helpful_text)  # Debugging line\n",
    "                    \n",
    "                    # Check if the text starts with a digit and extract the first contiguous digit sequence\n",
    "                    match = re.match(r'\\d+', helpful_text)\n",
    "                    if match:\n",
    "                        helpful_count = int(match.group())\n",
    "                    else:\n",
    "                        # If the text doesn't start with a digit, try converting the first word to a number\n",
    "                        helpful_count = w2n.word_to_num(helpful_text.split()[0])\n",
    "                else:\n",
    "                    print(\"Tag not found\")  # Debugging line\n",
    "                    helpful_count = 0\n",
    "\n",
    "                result['Customers_Influenced'] = helpful_count\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Handle the case where there is only one or no matching element\n",
    "                result['Customer_id_Critical_Review'] = 'None'\n",
    "                result['Customer_Name'] = 'None'\n",
    "                result['Customer_Review_Comment'] = 'None'\n",
    "                result['Customer_Review_Title'] = 'None'\n",
    "                result['Customers_Influenced'] = 0\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping extra parameters for {url}: {e}\")\n",
    "        traceback.print_exc()\n",
    "    return {}\n",
    "\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    seen_products = set()\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        for page in range(1, 2):\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-asin]\")))\n",
    "            except TimeoutException:\n",
    "                print(f\"Timed out waiting for elements on page {page} of category {category}.\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            for product in soup.find_all('div', attrs={\"data-asin\": True}):\n",
    "                product_dict = {}\n",
    "\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                item_name = product.find('span', class_='a-text-normal')\n",
    "                if item_name:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price:\n",
    "                    product_price = product_price.text.strip().replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "                    product_dict['price'] = product_price\n",
    "\n",
    "                rating_spans = product.find_all('span', attrs={\"aria-label\": True})\n",
    "                for rating_span in rating_spans:\n",
    "                    aria_label_value = rating_span.attrs[\"aria-label\"]\n",
    "                    if \"stars\" in aria_label_value:\n",
    "                        product_dict['ratings'] = aria_label_value.split(\" \")[0]\n",
    "                    else:\n",
    "                        if 'K+' in aria_label_value:\n",
    "                            product_dict['review_responders'] = aria_label_value\n",
    "                        else:\n",
    "                            try:\n",
    "                                int_value = int(aria_label_value)\n",
    "                                product_dict['review_responders'] = aria_label_value\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if item_reviews:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = process_review_count(reviews_text)\n",
    "                    product_dict['reviews'] = reviews_count\n",
    "\n",
    "\n",
    "                # Extract ASIN\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Construct the review URL using ASIN\n",
    "                if product_dict['Product_ID']:\n",
    "                    asin = product_dict['Product_ID']\n",
    "                    product_dict['url'] = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\"\n",
    "                    \n",
    "                else:\n",
    "                    product_dict['url'] = \"None\"\n",
    "\n",
    "\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                if 'Product_ID' in product_dict and product_dict['Product_ID']:\n",
    "                # Create a unique identifier for the product\n",
    "                    identifier = product_dict['Product_ID']\n",
    "\n",
    "                    if identifier not in seen_products:\n",
    "                        seen_products.add(identifier) #\n",
    "                        if product_dict.get('url'):\n",
    "                            extra_params = scrape_extra_parameters(product_dict['url'], driver)\n",
    "                            product_dict.update(extra_params)\n",
    "                        products.append(product_dict) #\n",
    "            all_products.extend(products)\n",
    "    driver.quit()\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    categories = {\n",
    "        'Smartphones': 'https://www.amazon.com/s?k=smartphone&ref=nb_sb_noss',\n",
    "        # 'Laptops': 'https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss',\n",
    "        # 'video_games': 'https://www.amazon.com/s?k=video_games&ref=nb_sb_noss',\n",
    "        # 'Dresses':'https://www.amazon.com/s?k=Dresses&ref=nb_sb_noss',\n",
    "        # 'Shoes':'https://www.amazon.com/s?k=Shoes&ref=nb_sb_noss',\n",
    "        # 'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&ref=nb_sb_noss',\n",
    "    }\n",
    "\n",
    "    all_products = []\n",
    "    try:\n",
    "        all_products = json.loads(scrape_amazon(categories))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during scraping: {e}\")\n",
    "    finally:\n",
    "        with open('amazon_data_ext.json', 'w') as file:\n",
    "            json.dump(all_products, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Formatted Post_Date: 2023-06-30\n",
      "INFO:root:Formatted Post_Date: 2023-03-07\n",
      "INFO:root:Formatted Post_Date: 2023-08-17\n",
      "INFO:root:Formatted Post_Date: 2023-04-24\n",
      "INFO:root:Formatted Post_Date: 2023-05-04\n",
      "INFO:root:Formatted Post_Date: 2023-07-13\n",
      "INFO:root:Formatted Post_Date: 2020-11-18\n",
      "INFO:root:Formatted Post_Date: 2023-09-22\n",
      "INFO:root:Formatted Post_Date: 2023-08-04\n",
      "INFO:root:Formatted Post_Date: 2022-08-04\n",
      "INFO:root:Formatted Post_Date: 2023-09-09\n",
      "INFO:root:Formatted Post_Date: 2023-09-14\n",
      "INFO:root:Formatted Post_Date: 2023-08-23\n",
      "INFO:root:Formatted Post_Date: 2023-08-23\n",
      "INFO:root:Formatted Post_Date: 2023-09-23\n",
      "INFO:root:Formatted Post_Date: 1677-09-21\n",
      "INFO:root:Formatted Post_Date: 2023-09-22\n",
      "INFO:root:Formatted Post_Date: 2022-11-16\n",
      "INFO:root:Formatted Post_Date: 2023-09-05\n",
      "INFO:root:Formatted Post_Date: 2023-09-07\n",
      "INFO:root:Formatted Post_Date: 2023-09-14\n",
      "INFO:root:Formatted Post_Date: 2023-08-26\n",
      "INFO:root:Formatted Post_Date: 2023-08-21\n",
      "INFO:root:Formatted Post_Date: 2023-09-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24 entries, 0 to 25\n",
      "Data columns (total 33 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Product_ID                   24 non-null     object \n",
      " 1   product                      24 non-null     object \n",
      " 2   price_dollars                24 non-null     float64\n",
      " 3   star_ratings                 24 non-null     float64\n",
      " 4   total_ratings                24 non-null     int32  \n",
      " 5   url                          24 non-null     object \n",
      " 6   category                     24 non-null     object \n",
      " 7   Customer_id_Critical_Review  24 non-null     object \n",
      " 8   Customer_Name                24 non-null     object \n",
      " 9   Customer_Review_Comment      24 non-null     object \n",
      " 10  Customer_Review_Title        24 non-null     object \n",
      " 11  Post_Date                    24 non-null     object \n",
      " 12  Customers_Influenced         24 non-null     float64\n",
      " 13  Customer_1_ID                24 non-null     object \n",
      " 14  Customer_1_Star_Rating       24 non-null     float64\n",
      " 15  Customer_1_Comment           24 non-null     object \n",
      " 16  Customer_1_buying_influence  24 non-null     float64\n",
      " 17  Customer_2_ID                24 non-null     object \n",
      " 18  Customer_2_Star_Rating       24 non-null     float64\n",
      " 19  Customer_2_Comment           24 non-null     object \n",
      " 20  Customer_2_buying_influence  24 non-null     float64\n",
      " 21  Customer_3_ID                24 non-null     object \n",
      " 22  Customer_3_Star_Rating       24 non-null     float64\n",
      " 23  Customer_3_Comment           24 non-null     object \n",
      " 24  Customer_3_buying_influence  24 non-null     float64\n",
      " 25  Customer_4_ID                24 non-null     object \n",
      " 26  Customer_4_Star_Rating       24 non-null     float64\n",
      " 27  Customer_4_Comment           24 non-null     object \n",
      " 28  Customer_4_buying_influence  24 non-null     float64\n",
      " 29  Customer_5_ID                24 non-null     object \n",
      " 30  Customer_5_Star_Rating       24 non-null     float64\n",
      " 31  Customer_5_Comment           24 non-null     object \n",
      " 32  Customer_5_buying_influence  24 non-null     float64\n",
      "dtypes: float64(13), int32(1), object(19)\n",
      "memory usage: 6.3+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_ext.json')\n",
    "\n",
    "\n",
    "# Replace NaN values in specified string columns with \"None\"\n",
    "string_columns = [\n",
    "    'Customer_1_ID',\n",
    "    'Customer_1_Comment',\n",
    "    'Customer_id_Critical_Review',\n",
    "    'Customer_Name',\n",
    "    'Customer_Review_Comment',\n",
    "    'Customer_Review_Title',\n",
    "    'Customer_2_ID'\n",
    "]\n",
    "df[string_columns] = df[string_columns].fillna(\"None\")\n",
    "\n",
    "# Replace NaN values in specified integer columns with 0\n",
    "integer_columns = [\n",
    "    'Customer_1_Star_Rating',\n",
    "    'Customer_1_buying_influence',\n",
    "    'Customers_Influenced'\n",
    "]\n",
    "df[integer_columns] = df[integer_columns].fillna(0)\n",
    "\n",
    "# Check if the 'Customers_Influenced' column is in the DataFrame\n",
    "if 'Customers_Influenced' not in df.columns:\n",
    "    print(\"Column 'Customers_Influenced' not found in the DataFrame. Please check the column name in the JSON file.\")\n",
    "\n",
    "# Convert 'Post_Date' to datetime object and handle any conversion errors by setting them to NaT\n",
    "df['Post_Date'] = pd.to_datetime(df['Post_Date'], errors='coerce', format='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Replace NaT values with a date within bounds or use NaT\n",
    "df['Post_Date'].fillna(pd.NaT, inplace=True)\n",
    "\n",
    "# Convert 'Post_Date' to 'yyyy-mm-dd' string format for PostgreSQL\n",
    "df['Post_Date'] = df['Post_Date'].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else '1677-09-21')\n",
    "\n",
    "\n",
    "\n",
    "# Replace NaN values with 'None' in the specified columns\n",
    "df['Customer_id_Critical_Review'] = df['Customer_id_Critical_Review'].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "df['Customer_Name'] = df['Customer_Name'].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "df['Customer_Review_Comment'] = df['Customer_Review_Comment'].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "df['Customer_Review_Title'] = df['Customer_Review_Title'].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "df['Customers_Influenced'] = df['Customers_Influenced'].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "\n",
    "# Define the columns to be updated\n",
    "columns_to_update = [f'Customer_{i}_Comment' for i in range(1, 6)]\n",
    "for column in columns_to_update:\n",
    "    df[column] = df[column].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "\n",
    "columns_to_update = [f'Customer_{i}_ID' for i in range(1, 6)]\n",
    "for column in columns_to_update:\n",
    "    df[column] = df[column].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "\n",
    "# Drop all items with review having zero values\n",
    "df.drop(df.index[df['Customer_1_ID'] == 'None'], inplace=True)\n",
    "\n",
    "# Define the columns to be updated\n",
    "new_columns = ['Customer_id_Critical_Review', 'Customer_Name', 'Customer_Review_Comment', 'Customer_Review_Title', 'Customers_Influenced']\n",
    "for column in new_columns:\n",
    "    if column == 'Customers_Influenced': df[column] = df[column].replace({'\"NaN\"': 'None', 'NaN': 'None'})\n",
    "\n",
    "# Drop the 'review_responders' column if it exists\n",
    "if 'review_responders' in df.columns:\n",
    "    df.drop(columns=['review_responders'], inplace=True)\n",
    "\n",
    "# Handle other columns similarly\n",
    "df['price'] = df['price'].apply(lambda x: float(x) if pd.notnull(x) else None)\n",
    "df['ratings'] = df['ratings'].apply(lambda x: float(x) if pd.notnull(x) else None)\n",
    "df['reviews'] = df['reviews'].fillna(0).astype(int)\n",
    "\n",
    "# Drop rows where the 'price' column is NaN\n",
    "df.dropna(subset=['price'], inplace=True)\n",
    "\n",
    "# Drop all items with review having zero values or Customer_1_ID as 'None'\n",
    "df.drop(df.index[(df['reviews'] == 0) | (df['Customer_1_ID'] == 'None')], inplace=True)\n",
    "\n",
    "################################\n",
    "# df.info()\n",
    "# print(df.tail())\n",
    "################################\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\",\n",
    "    client_encoding='utf8'\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Modify the CREATE TABLE query to include additional columns\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_data_ext;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data_ext (\n",
    "    product_id TEXT NOT NULL,\n",
    "    product TEXT NOT NULL,\n",
    "    star_ratings NUMERIC NULL,\n",
    "    price_dollars NUMERIC NULL,\n",
    "    total_ratings INTEGER NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    url TEXT NOT NULL,\n",
    "    Customer_id_Critical_Review TEXT,\n",
    "    Customer_Name TEXT,\n",
    "    Post_Date DATE,\n",
    "    Customer_Review_Comment TEXT,\n",
    "    Customer_Review_Title TEXT,\n",
    "    Customers_Influenced INTEGER,\n",
    "    \"\"\" + \",\\n    \".join([f\"Customer_{i}_ID TEXT, Customer_{i}_Star_Rating NUMERIC, Customer_{i}_Comment TEXT, Customer_{i}_buying_influence INTEGER\" for i in range(1, 6)]) + \"\"\"\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "def clean_format_data(row):\n",
    "    # Extract the values directly, as they are already cleaned\n",
    "    ratings = row['ratings']\n",
    "    price = row['price']\n",
    "    reviews = row['reviews']\n",
    "    product_id = row['Product_ID']\n",
    "    product = psycopg2.extensions.adapt(str(row['product']).encode('utf-8', 'replace')).getquoted().decode('utf-8')[1:-1]\n",
    "    category = psycopg2.extensions.adapt(row['category'].encode('utf-8', 'replace')).getquoted().decode('utf-8')[1:-1]\n",
    "    url = row['url']\n",
    "      \n",
    "    critical_review_id = row['Customer_id_Critical_Review'] if row['Customer_id_Critical_Review'] != 'None' else None\n",
    "    customer_name = row['Customer_Name'] if row['Customer_Name'] != 'None' else None\n",
    "    customer_review_comment = row['Customer_Review_Comment'] if row['Customer_Review_Comment'] != 'None' else None\n",
    "    customer_review_title = row['Customer_Review_Title'] if row['Customer_Review_Title'] != 'None' else None\n",
    "    customers_influenced = row['Customers_Influenced'] if row['Customers_Influenced'] != 'None' else None  # Correctly handle NaN values\n",
    "    \n",
    "    # Handle additional customer information\n",
    "    customer_data = []\n",
    "    for i in range(1, 6):\n",
    "        customer_id = row[f'Customer_{i}_ID'] if row[f'Customer_{i}_ID'] != 'None' else None\n",
    "        star_rating = row[f'Customer_{i}_Star_Rating'] if pd.notna(row[f'Customer_{i}_Star_Rating']) else None\n",
    "        comment = psycopg2.extensions.adapt(str(row[f'Customer_{i}_Comment']).encode('utf-8', 'replace')).getquoted().decode('utf-8')[1:-1]\n",
    "        buying_influence = row[f'Customer_{i}_buying_influence'] if pd.notna(row[f'Customer_{i}_buying_influence']) else None\n",
    "\n",
    "        customer_data.extend([customer_id, star_rating, comment, buying_influence])\n",
    "\n",
    "    # Validate and format the 'Post_Date' before returning\n",
    "\n",
    "    # Validate and format the 'Post_Date' before returning\n",
    "    post_date = row['Post_Date']\n",
    "    if post_date is not None and isinstance(post_date, str):\n",
    "        try:\n",
    "            # Check if post_date is already in 'YYYY-MM-DD' format\n",
    "            if re.match(r'\\d{4}-\\d{2}-\\d{2}', post_date):\n",
    "                datetime.strptime(post_date, '%Y-%m-%d')  # Validate the format\n",
    "            else:\n",
    "                # Try to parse the post_date to a datetime object with a different format\n",
    "                parsed_date = datetime.strptime(post_date, '%Y-%m-%dT%H:%M:%S')\n",
    "                # Format it back to a string in the desired format\n",
    "                post_date = parsed_date.strftime('%Y-%m-%d')\n",
    "        except ValueError as ve:\n",
    "            logging.error(f\"Invalid date format for Post_Date: {post_date}. Setting it to default value.\")\n",
    "            post_date = \"0001-01-01\"  # Default value for invalid date format\n",
    "    else:\n",
    "        logging.error(f\"Post_Date is not a string or is None: {post_date}. Setting it to default value.\")\n",
    "        post_date = \"0001-01-01\"  # Default value for None or non-string values\n",
    "    \n",
    "    # Replace other None values with appropriate default values\n",
    "    critical_review_id = critical_review_id if critical_review_id is not None else \"Unavailable\"\n",
    "    customer_name = customer_name if customer_name is not None else \"Unavailable\"\n",
    "    customer_review_comment = customer_review_comment if customer_review_comment is not None else \"Unavailable\"\n",
    "    customer_review_title = customer_review_title if customer_review_title is not None else \"Unavailable\"\n",
    "    customers_influenced = customers_influenced if customers_influenced is not None else 0  # Default numeric value\n",
    "    \n",
    "    # Handle additional customer information\n",
    "    customer_data = []\n",
    "    for i in range(1, 6):\n",
    "        customer_id = row[f'Customer_{i}_ID'] if row[f'Customer_{i}_ID'] != 'None' else \"Unavailable\"\n",
    "        star_rating = row[f'Customer_{i}_Star_Rating'] if pd.notna(row[f'Customer_{i}_Star_Rating']) else 0.0\n",
    "        comment = psycopg2.extensions.adapt(str(row[f'Customer_{i}_Comment']).encode('utf-8', 'replace')).getquoted().decode('utf-8')[1:-1]\n",
    "        comment = comment if comment != 'None' else \"Unavailable\"\n",
    "        buying_influence = row[f'Customer_{i}_buying_influence'] if pd.notna(row[f'Customer_{i}_buying_influence']) else 0\n",
    "        \n",
    "        customer_data.extend([customer_id, star_rating, comment, buying_influence])\n",
    "\n",
    "    logging.info(f\"Formatted Post_Date: {post_date}\")  # Log the formatted post_date\n",
    "    \n",
    "    return product_id, product, price, ratings, reviews, category, url, critical_review_id, customer_name, post_date, customer_review_comment, customer_review_title, customers_influenced, *customer_data\n",
    "\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO amazon_data_ext (\n",
    "    product_id, product, price_dollars, star_ratings, total_ratings, category, url,\n",
    "    Customer_id_Critical_Review, Customer_Name, Post_Date, Customer_Review_Comment, Customer_Review_Title, Customers_Influenced,\n",
    "    \"\"\" + \", \".join([f\"Customer_{i}_ID, Customer_{i}_Star_Rating, Customer_{i}_Comment, Customer_{i}_buying_influence\" for i in range(1, 6)]) + \"\"\"\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, \"\"\" + \", \".join([\"%s\"] * 20) + \")\"\n",
    "\n",
    "\n",
    "# Insert the data from the pandas DataFrame into the PostgreSQL table\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        cur.execute(insert_query, clean_format_data(row))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting row at index {index}: {e}\")\n",
    "        logging.error(f\"Row data: {row}\")  # Log the entire row data\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df.rename(columns={'ratings': 'star_ratings', 'reviews': 'total_ratings', 'price': 'price_dollars'}, inplace=True)\n",
    "\n",
    "# Define the base order of columns\n",
    "column_order = [\n",
    "    \"Product_ID\", \"product\", \"price_dollars\", \"star_ratings\", \n",
    "    \"total_ratings\", \"url\", \"category\", \"Customer_id_Critical_Review\", \n",
    "    \"Customer_Name\", \"Customer_Review_Comment\", \"Customer_Review_Title\", \n",
    "    \"Post_Date\", \"Customers_Influenced\"\n",
    "]\n",
    "\n",
    "# Add the customer related columns to the order list\n",
    "for i in range(1, 6):\n",
    "    column_order.extend([\n",
    "        f\"Customer_{i}_ID\", f\"Customer_{i}_Star_Rating\", \n",
    "        f\"Customer_{i}_Comment\", f\"Customer_{i}_buying_influence\"\n",
    "    ])\n",
    "\n",
    "# Reorder the columns\n",
    "df = df.reindex(columns=column_order)\n",
    "\n",
    "df.info()\n",
    "# Save the DataFrame to a CSV file with updated column names\n",
    "df.to_csv('amazon_data_ext.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
