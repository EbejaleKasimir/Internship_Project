{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2537593994.py, line 334)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 334\u001b[1;36m\u001b[0m\n\u001b[1;33m    after creating the json file inside are a list of product_ID asin. The information can be used populating the url below. These url pages have an item information I wish to scrape. Using the concept above using the scrape_extra_parameters function and extract_specific_review function details I should be able to get the information the links below. Because the elements are similar, just the url is different.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import traceback\n",
    "import logging\n",
    "import re  # Make sure to include this import\n",
    "from word2number import w2n\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_review_count(text):\n",
    "    text = text.strip().replace(',', '')\n",
    "    if 'K+' in text:\n",
    "        return str(int(float(text.replace('(', '').replace(')', '').replace('K+', '').strip()) * 1000))\n",
    "    return text\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    try:\n",
    "        driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Failed to install Edge Chromium driver.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "def scrape_extra_parameters(url: str, driver: webdriver.Edge) -> dict:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"TimeoutException: Could not find reviews for {url}\")\n",
    "            return {}\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract the general reviews\n",
    "        reviews_tags = soup.find_all('div', attrs={'data-hook': 'review'})\n",
    "\n",
    "        result = {}\n",
    "        for i, review_tag in enumerate(reviews_tags[:5]):\n",
    "            result[f'Customer_{i + 1}_ID'] = review_tag.attrs.get('id', 'None')\n",
    "            \n",
    "            # Extract the Star Rating\n",
    "            star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating\"] span.a-icon-alt')\n",
    "            star_rating = float(star_rating_tag.text.split()[0]) if star_rating_tag else 0.0\n",
    "            result[f'Customer_{i+1}_Star_Rating'] = star_rating\n",
    "            \n",
    "            # Extract the Comment Title\n",
    "            comment_title_tag = review_tag.select_one('a[data-hook=\"review-title\"]')\n",
    "            # Inside the for loop, after extracting the comment title:\n",
    "            if comment_title_tag:\n",
    "                actual_comment_title = comment_title_tag.text.strip()\n",
    "            else:\n",
    "                # Handle alternate structure\n",
    "                comment_title_tag = review_tag.select_one('span.cr-original-review-content')\n",
    "                actual_comment_title = comment_title_tag.text.strip() if comment_title_tag else 'NaN'\n",
    "\n",
    "            # Remove the pattern \"k out of 5 stars\\n\" from the comment\n",
    "            actual_comment_title = re.sub(r'\\d+(\\.\\d+)? out of 5 stars\\n', '', actual_comment_title)\n",
    "\n",
    "            result[f'Customer_{i+1}_Comment'] = actual_comment_title\n",
    "\n",
    "            # Extract the Number of people who found the review helpful\n",
    "            helpful_vote_tag = review_tag.select_one('span[data-hook=\"helpful-vote-statement\"]')\n",
    "            helpful_count = w2n.word_to_num(helpful_vote_tag.text.split()[0]) if helpful_vote_tag else 0\n",
    "            result[f'Customer_{i+1}_buying_influence'] = helpful_count\n",
    "        \n",
    "\n",
    "        # Extract Top Positive and Critical Reviews (Moved outside of the above loop)\n",
    "        Parent_review_tags = soup.select('div[id^=\"viewpoint-\"]')\n",
    "        if len(Parent_review_tags) > 0: \n",
    "            ts = 'positive-review'\n",
    "            result.update(extract_specific_review(Parent_review_tags[0], 'Top_Positive', ts, soup, url))\n",
    "\n",
    "        else:\n",
    "            result.update(set_default_values('Top_Positive'))\n",
    "            \n",
    "        if len(Parent_review_tags) > 1: \n",
    "            ts = 'critical-review.a-span-last'\n",
    "            result.update(extract_specific_review(Parent_review_tags[1], 'Critical', ts, soup, url))\n",
    "\n",
    "        else:\n",
    "            result.update(set_default_values('Critical'))\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping extra parameters: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def extract_specific_review(review_tag, review_type, ts, soup, url):\n",
    "    specific_result = {}\n",
    "    \n",
    "    # Extracting ID\n",
    "    review_id = review_tag.get('id', 'None').replace('viewpoint-', '')\n",
    "    specific_result[f'{review_type}_Review_Cust_ID'] = review_id\n",
    "\n",
    "    # # Extract Customer Name and Influenced\n",
    " \n",
    "     # Corrected Extraction for Customer Name\n",
    "    customer_name_selector = 'div.a-profile-content span.a-profile-name'\n",
    "    specific_result[f'{review_type}_Review_Cust_Name'] = review_tag.select_one(customer_name_selector).text if review_tag.select_one(customer_name_selector) else 'None'\n",
    "\n",
    "    # Corrected Selector\n",
    "    influenced_selector = f'div.a-column.a-span6.view-point-review.{ts} div.a-row.a-spacing-top-small span.a-size-small.a-color-tertiary span.review-votes'\n",
    "    influenced_element = soup.select_one(influenced_selector)\n",
    "\n",
    "    if influenced_element:\n",
    "        # Directly extract the text from the found element\n",
    "        helpful_text = influenced_element.text.strip()\n",
    "        print(\"Helpful Text:\", helpful_text)  # Debugging line\n",
    "        \n",
    "        # Check if the text starts with a digit and extract the first contiguous digit sequence\n",
    "        match = re.match(r'\\d+', helpful_text)\n",
    "        if match:\n",
    "            helpful_count = int(match.group())\n",
    "        else:\n",
    "            # If the text doesn't start with a digit, try converting the first word to a number\n",
    "            helpful_count = w2n.word_to_num(helpful_text.split()[0])\n",
    "    else:\n",
    "        print(f\"Tag not found in {url}\")  # Debugging line\n",
    "        helpful_count = 0\n",
    "\n",
    "    specific_result[f'{review_type}_Review_Cust_Influenced'] = helpful_count\n",
    "\n",
    "    \n",
    "    # Extract Customer Review Comment\n",
    "    review_comment_tag = review_tag.find('div', class_='a-row a-spacing-top-mini')\n",
    "    specific_result[f'{review_type}_Review_Cust_Comment'] = review_comment_tag.text.strip() if review_comment_tag else 'None'\n",
    "    \n",
    "    # Extract Customer Review Title\n",
    "    review_title_tag = review_tag.select_one('span[data-hook=\"review-title\"]')\n",
    "    specific_result[f'{review_type}_Review_Cust_Comment_Title'] = review_title_tag.text if review_title_tag else 'None'\n",
    "    \n",
    "    # Extract the post time\n",
    "    review_tags_date = review_tag.select('div.a-expander-content.a-expander-partial-collapse-content span.a-size-base.a-color-secondary.review-date')\n",
    "    if review_tags_date:\n",
    "        post_time_text = review_tags_date[0].text.strip()\n",
    "        match = re.search(r'on (.+)$', post_time_text)\n",
    "        if match:\n",
    "            date_string = match.group(1)\n",
    "            try:\n",
    "                post_date = datetime.strptime(date_string, '%B %d, %Y')\n",
    "                specific_result[f'{review_type}_Review_Cust_Date'] = post_date.isoformat()                            \n",
    "            except ValueError as ve:\n",
    "                print(f\"Error parsing date string {date_string}: {ve}\")\n",
    "                specific_result[f'{review_type}_Review_Cust_Date'] = '-'\n",
    "        else:\n",
    "            print(\"Date not found in text:\", post_time_text)\n",
    "            specific_result[f'{review_type}_Review_Cust_Date'] = '-'\n",
    "    else:\n",
    "        print(\"Date tag not found\")\n",
    "        specific_result[f'{review_type}_Review_Cust_Date'] = None\n",
    "    \n",
    "    \n",
    "    # Extract the Star Rating\n",
    "    review_star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating-view-point\"] span.a-icon-alt')\n",
    "    star_rating = float(review_star_rating_tag.text.split()[0]) if review_star_rating_tag else 0.0\n",
    "    specific_result[f'{review_type}_Review_Cust_Star_Rating'] = star_rating\n",
    "    \n",
    "    return specific_result\n",
    "\n",
    "def set_default_values(review_type):\n",
    "    default_values = {\n",
    "        f'{review_type}_Review_Cust_ID': 'None',\n",
    "        f'{review_type}_Review_Cust_Name': 'None',\n",
    "        f'{review_type}_Review_Cust_Comment': 'None',\n",
    "        f'{review_type}_Review_Cust_Comment_Title': 'None',\n",
    "        f'{review_type}_Review_Cust_Influenced': 0,\n",
    "        f'{review_type}_Review_Cust_Star_Rating': 0.0,\n",
    "        f'{review_type}_Review_Cust_Date': None,\n",
    "    }\n",
    "    return default_values\n",
    "\n",
    "    #     return result\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error scraping extra parameters for {url}: {e}\")\n",
    "    #     traceback.print_exc()\n",
    "    # return {}\n",
    "\n",
    "def scrape_amazon(categories):\n",
    " \n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    seen_products = set()\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        for page in range(1, 10):\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#search > div.s-desktop-width-max.s-desktop-content.s-wide-grid-style-t1.s-opposite-dir.s-wide-grid-style.sg-row > div.sg-col-20-of-24.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span.rush-component.s-latency-cf-section > div.s-main-slot.s-result-list.s-search-results.sg-row > div:nth-child(1)\")))\n",
    "            except TimeoutException:\n",
    "                print(f\"Timed out waiting for elements on page {page} of category {category}.\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find all products using the given CSS selector\n",
    "            products_tags = soup.select(\"#search > div.s-desktop-width-max.s-desktop-content.s-wide-grid-style-t1.s-opposite-dir.s-wide-grid-style.sg-row > div.sg-col-20-of-24.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span.rush-component.s-latency-cf-section > div.s-main-slot.s-result-list.s-search-results.sg-row > div\")\n",
    "\n",
    "            products_list = []  # Use a different name for the list of product_dict dictionaries\n",
    "\n",
    "            for product in products_tags:\n",
    "                product_dict = {}\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Try to find item_name with the general class\n",
    "                item_name = product.find('span', class_='a-text-normal')\n",
    "\n",
    "                # If not found, try the first specific class\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If still not found, try the second specific class\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                # If item_name is found with any class, extract the text\n",
    "                if item_name:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "                else:\n",
    "                    print(f\"Failed to scrape item name in {url}\")\n",
    "                    product_dict['product'] = \"Unknown\"\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price:\n",
    "                    product_price = product_price.text.strip().replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "                    product_dict['price'] = product_price\n",
    "\n",
    "                rating_spans = product.find_all('span', attrs={\"aria-label\": True})\n",
    "                for rating_span in rating_spans:\n",
    "                    aria_label_value = rating_span.attrs[\"aria-label\"]\n",
    "                    if \"stars\" in aria_label_value:\n",
    "                        product_dict['ratings'] = aria_label_value.split(\" \")[0]\n",
    "                    else:\n",
    "                        if 'K+' in aria_label_value:\n",
    "                            product_dict['review_responders'] = aria_label_value\n",
    "                        else:\n",
    "                            try:\n",
    "                                int_value = int(aria_label_value)\n",
    "                                product_dict['review_responders'] = aria_label_value\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if item_reviews:\n",
    "                    try:\n",
    "                        reviews_text = item_reviews.text.strip()\n",
    "                        reviews_count = process_review_count(reviews_text)\n",
    "                        product_dict['reviews'] = reviews_count\n",
    "                        logging.info(f\"Successfully scraped total {product_dict['reviews']} rating for product {product_dict.get('Product_ID', 'Unknown ID')}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing review count for product {product_dict.get('Product_ID', 'Unknown ID')}: {e}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Failed to scrape total rating for product {product_dict.get('Product_ID', 'Unknown ID')}\")\n",
    "\n",
    "            \n",
    "\n",
    "                # Extract ASIN\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Construct the review URL using ASIN\n",
    "                if product_dict['Product_ID']:\n",
    "                    asin = product_dict['Product_ID']\n",
    "                    product_dict['url'] = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\"\n",
    "                    \n",
    "                else:\n",
    "                    product_dict['url'] = None\n",
    "\n",
    "\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                if 'Product_ID' in product_dict and product_dict['Product_ID']:\n",
    "                # Create a unique identifier for the product\n",
    "                    identifier = product_dict['Product_ID']\n",
    "\n",
    "                    if identifier not in seen_products:\n",
    "                        seen_products.add(identifier) #\n",
    "                        if product_dict.get('url'):\n",
    "                            extra_params = scrape_extra_parameters(product_dict['url'], driver)\n",
    "                            product_dict.update(extra_params)\n",
    "                            products_list.append(product_dict)\n",
    "\n",
    "                        all_products.extend(products_list)\n",
    "    driver.quit()\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    categories = {\n",
    "        'Smartphones': 'https://www.amazon.com/s?k=smartphone&ref=nb_sb_noss',\n",
    "        'Laptops': 'https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss',\n",
    "        'video_games': 'https://www.amazon.com/s?k=video_games&ref=nb_sb_noss',\n",
    "        'Dresses':'https://www.amazon.com/s?k=Dresses&ref=nb_sb_noss',\n",
    "        'Shoes':'https://www.amazon.com/s?k=Shoes&ref=nb_sb_noss',\n",
    "        'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&ref=nb_sb_noss',\n",
    "    }\n",
    "\n",
    "    all_products = []\n",
    "    try:\n",
    "        all_products = json.loads(scrape_amazon(categories))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during scraping: {e}\")\n",
    "    finally:\n",
    "        with open('amazon_data_ext.json', 'w') as file:\n",
    "            json.dump(all_products, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:WDM:====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:There is no [win64] edgedriver \"117.0.2045.47\" for browser edge \"117.0.2045\" in cache\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:About to download new driver from https://msedgedriver.azureedge.net/117.0.2045.47/edgedriver_win64.zip\n",
      "INFO:WDM:Driver downloading response is 200\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Driver has been saved in cache [C:\\Users\\Kasim\\.wdm\\drivers\\edgedriver\\win64\\117.0.2045.47]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from word2number import w2n\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the previous JSON\n",
    "with open('amazon_data_ext.json', 'r') as file:\n",
    "    all_products = json.load(file)\n",
    "\n",
    "def extract_total_ratings(soup, selector):\n",
    "    \"\"\"\n",
    "    Extracts the total ratings from the provided soup object using the given selector.\n",
    "    \"\"\"\n",
    "    total_ratings_element = soup.select_one(selector)\n",
    "    if total_ratings_element:\n",
    "        match = re.search(r'(\\d+) total ratings', total_ratings_element.text)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def extract_individual_review(asin, driver, review_type):\n",
    "    if review_type == \"positive\":\n",
    "        url = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_arp_d_viewpnt_lft?ie=UTF8&reviewerType=all_reviews&filterByStar=positive&pageNumber=1\"\n",
    "    else:\n",
    "        url = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_arp_d_viewpnt_rgt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber=1\"\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        result = {'ASIN': asin}\n",
    "        \n",
    "        # For individual positive or negative reviews\n",
    "        reviews_tags = soup.find_all('div', attrs={'data-hook': 'review'})\n",
    "        if reviews_tags:\n",
    "            review_tag = reviews_tags[0]\n",
    "            result['Review_Type'] = review_type\n",
    "            result.update(extract_specific_review(review_tag, review_type.capitalize(), 'positive-review' if review_type == 'positive' else 'critical-review.a-span-last', soup, url))\n",
    "        \n",
    "        if review_type == \"positive\":\n",
    "            result['Total_Positive_Ratings'] = extract_total_ratings(soup, '#filter-info-section > div.a-row.a-spacing-base.a-size-base')\n",
    "        else:\n",
    "            result['Total_Negative_Ratings'] = extract_total_ratings(soup, '#filter-info-section > div.a-row.a-spacing-base.a-size-base')\n",
    "\n",
    "        return result\n",
    "         \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping individual {review_type} review for ASIN {asin}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return {}\n",
    "\n",
    "def extract_top_reviews(asin, driver, review_type):\n",
    "    if review_type == \"positive\":\n",
    "        url = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_arp_d_viewpnt_lft?ie=UTF8&reviewerType=all_reviews&filterByStar=positive&pageNumber=1\"\n",
    "    else:\n",
    "        url = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_arp_d_viewpnt_rgt?ie=UTF8&reviewerType=all_reviews&filterByStar=critical&pageNumber=1\"\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        result = {'ASIN': asin}\n",
    "        \n",
    "        ratings_count = extract_total_ratings(soup, '#filter-info-section > div.a-row.a-spacing-base.a-size-base')\n",
    "        if review_type == \"positive\":\n",
    "            result['Total_Positive_Ratings'] = ratings_count\n",
    "        else:\n",
    "            result['Total_Negative_Ratings'] = ratings_count\n",
    "\n",
    "        return result\n",
    "         \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping top reviews for ASIN {asin}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "\n",
    "def process_review_count(text):\n",
    "    text = text.strip().replace(',', '')\n",
    "    if 'K+' in text:\n",
    "        return str(int(float(text.replace('(', '').replace(')', '').replace('K+', '').strip()) * 1000))\n",
    "    return text\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    try:\n",
    "        driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Failed to install Edge Chromium driver.\")\n",
    "    return driver\n",
    "\n",
    "def extract_specific_review(review_tag, review_type, ts, soup, url):\n",
    "    specific_result = {}\n",
    "    \n",
    "    # Extracting ID\n",
    "    review_id = review_tag.get('id', 'None').replace('viewpoint-', '')\n",
    "    specific_result[f'{review_type}_Review_Cust_ID'] = review_id\n",
    "\n",
    "    # Extract Customer Name\n",
    "    customer_name_selector = 'div.a-profile-content span.a-profile-name'\n",
    "    customer_name = review_tag.select_one(customer_name_selector).text if review_tag.select_one(customer_name_selector) else 'None'\n",
    "    specific_result[f'{review_type}_Review_Cust_Name'] = customer_name\n",
    "\n",
    "    # Extract Influenced\n",
    "    influenced_selector = f'div.a-column.a-span6.view-point-review.{ts} div.a-row.a-spacing-top-small span.a-size-small.a-color-tertiary span.review-votes'\n",
    "    influenced_element = soup.select_one(influenced_selector)\n",
    "    if influenced_element:\n",
    "        helpful_text = influenced_element.text.strip()\n",
    "        match = re.match(r'\\d+', helpful_text)\n",
    "        if match:\n",
    "            helpful_count = int(match.group())\n",
    "        else:\n",
    "            helpful_count = w2n.word_to_num(helpful_text.split()[0])\n",
    "    else:\n",
    "        helpful_count = 0\n",
    "    specific_result[f'{review_type}_Review_Cust_Influenced'] = helpful_count\n",
    "\n",
    "    return specific_result\n",
    "\n",
    "# Main Scraper Function\n",
    "def scrape_positive_and_negative_reviews():\n",
    "    driver = setup_driver()\n",
    "    all_reviews = []\n",
    "\n",
    "    for product in all_products:\n",
    "        asin = product.get('Product_ID')\n",
    "        if asin:\n",
    "            # Create a base dictionary for this ASIN\n",
    "            combined_review = {'ASIN': asin}\n",
    "\n",
    "            # Extract positive top reviews\n",
    "            positive_top_reviews = extract_top_reviews(asin, driver, \"positive\")\n",
    "            if positive_top_reviews:\n",
    "                # Remove the ASIN key from the result and update the combined_review\n",
    "                positive_top_reviews.pop('ASIN', None)\n",
    "                combined_review.update(positive_top_reviews)\n",
    "            \n",
    "            # Extract negative top reviews\n",
    "            negative_top_reviews = extract_top_reviews(asin, driver, \"negative\")\n",
    "            if negative_top_reviews:\n",
    "                # Remove the ASIN key from the result and update the combined_review\n",
    "                negative_top_reviews.pop('ASIN', None)\n",
    "                combined_review.update(negative_top_reviews)\n",
    "\n",
    "            # Append only if combined_review has more than just the ASIN\n",
    "            if len(combined_review) > 1:\n",
    "                all_reviews.append(combined_review)\n",
    "    \n",
    "    driver.quit()\n",
    "    return all_reviews\n",
    "\n",
    "\n",
    "def remove_duplicates_from_output():\n",
    "    with open('amazon_positive_negative_reviews.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    unique_data = {review['ASIN']: review for review in data}.values()\n",
    "\n",
    "    with open('amazon_positive_negative_reviews.json', 'w') as f:\n",
    "        json.dump(list(unique_data), f)\n",
    "\n",
    "# Execution\n",
    "if __name__ == '__main__':\n",
    "    # Load the previous JSON\n",
    "    with open('amazon_data_ext.json', 'r') as file:\n",
    "        all_products = json.load(file)\n",
    "\n",
    "    all_reviews = scrape_positive_and_negative_reviews()\n",
    "\n",
    "    # Save the extracted reviews to a new JSON file\n",
    "    with open('amazon_positive_negative_reviews.json', 'w') as file:\n",
    "        json.dump(all_reviews, file)\n",
    "\n",
    "    # Remove duplicates from the output file\n",
    "    remove_duplicates_from_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
