{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:WDM:====== WebDriver manager ======\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:There is no [win64] edgedriver \"117.0.2045.47\" for browser edge \"117.0.2045\" in cache\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:About to download new driver from https://msedgedriver.azureedge.net/117.0.2045.47/edgedriver_win64.zip\n",
      "INFO:WDM:Driver downloading response is 200\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Driver has been saved in cache [C:\\Users\\Kasim\\.wdm\\drivers\\edgedriver\\win64\\117.0.2045.47]\n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "INFO:root:Successfully scraped total  rating for product B0CGT546WH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0CBD6SR44\n",
      "INFO:root:Successfully scraped total  rating for product B0BLW25MB7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException: Could not find reviews for https://www.amazon.com/product-reviews/B0CBD6SR44/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0B2D77YB8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 9 people found this helpful\n",
      "Helpful Text: 12 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0CHMGFKP6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 17 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product \n",
      "INFO:root:Successfully scraped total  rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "INFO:root:Successfully scraped total  rating for product B0BWSG8VDK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0C7D17Y68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 12 people found this helpful\n",
      "Helpful Text: 12 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0BY3PGDZR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0C7D17Y68/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Helpful Text: 8 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0C7RZJ5N4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 8 people found this helpful\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0BY3PGDZR/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0BZ8X9HGT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 16 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0CB5NZRZM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 7 people found this helpful\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0BZ8X9HGT/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to scrape total rating for product B0CHN3YKW8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0CB5NZRZM/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Helpful Text: 2 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0CCY5J8WF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException: Could not find reviews for https://www.amazon.com/product-reviews/B0CHN3YKW8/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0CF54XF86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: 2 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0BWHJQMKH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: One person found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B09W5691HJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 199 people found this helpful\n",
      "Helpful Text: 3 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0C79D6V19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B09W5691HJ/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Helpful Text: One person found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0BS4BP8FB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: 6 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kasim\\AppData\\Local\\Temp\\ipykernel_4192\\688144476.py\", line 82, in scrape_extra_parameters\n",
      "    helpful_count = w2n.word_to_num(helpful_vote_tag.text.split()[0]) if helpful_vote_tag else 0\n",
      "  File \"C:\\Users\\Kasim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\word2number\\w2n.py\", line 154, in word_to_num\n",
      "    raise ValueError(\"No valid number words found! Please enter a valid number word (eg. two million twenty three thousand and forty nine)\")\n",
      "ValueError: No valid number words found! Please enter a valid number word (eg. two million twenty three thousand and forty nine)\n",
      "INFO:root:Successfully scraped total  rating for product B0CC6HMF7N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping extra parameters: No valid number words found! Please enter a valid number word (eg. two million twenty three thousand and forty nine)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0C9QV678Q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0CC6HMF7N/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0CC6HMF7N/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0C3HR6XDK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0C9QV678Q/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Helpful Text: 14 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product B0C6YRCFM2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 11 people found this helpful\n",
      "Helpful Text: 10 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total  rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0C6YRCFM2/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\n",
      "Helpful Text: 4 people found this helpful\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import traceback\n",
    "import logging\n",
    "import re  # Make sure to include this import\n",
    "from word2number import w2n\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_review_count(text):\n",
    "    text = text.strip().replace(',', '')\n",
    "    if 'K+' in text:\n",
    "        return str(int(float(text.replace('(', '').replace(')', '').replace('K+', '').strip()) * 1000))\n",
    "    return text\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    try:\n",
    "        driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Failed to install Edge Chromium driver.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "def scrape_extra_parameters(url: str, driver: webdriver.Edge) -> dict:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"TimeoutException: Could not find reviews for {url}\")\n",
    "            return {}\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract the general reviews\n",
    "        reviews_tags = soup.find_all('div', attrs={'data-hook': 'review'})\n",
    "\n",
    "        result = {}\n",
    "        for i, review_tag in enumerate(reviews_tags[:5]):\n",
    "            result[f'Customer_{i + 1}_ID'] = review_tag.attrs.get('id', 'None')\n",
    "            \n",
    "            # Extract the Star Rating\n",
    "            star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating\"] span.a-icon-alt')\n",
    "            star_rating = float(star_rating_tag.text.split()[0]) if star_rating_tag else 0.0\n",
    "            result[f'Customer_{i+1}_Star_Rating'] = star_rating\n",
    "            \n",
    "            # Extract the Comment Title\n",
    "            comment_title_tag = review_tag.select_one('a[data-hook=\"review-title\"]')\n",
    "            # Inside the for loop, after extracting the comment title:\n",
    "            if comment_title_tag:\n",
    "                actual_comment_title = comment_title_tag.text.strip()\n",
    "            else:\n",
    "                # Handle alternate structure\n",
    "                comment_title_tag = review_tag.select_one('span.cr-original-review-content')\n",
    "                actual_comment_title = comment_title_tag.text.strip() if comment_title_tag else 'NaN'\n",
    "\n",
    "            # Remove the pattern \"k out of 5 stars\\n\" from the comment\n",
    "            actual_comment_title = re.sub(r'\\d+(\\.\\d+)? out of 5 stars\\n', '', actual_comment_title)\n",
    "\n",
    "            result[f'Customer_{i+1}_Comment'] = actual_comment_title\n",
    "\n",
    "            # Extract the Number of people who found the review helpful\n",
    "            helpful_vote_tag = review_tag.select_one('span[data-hook=\"helpful-vote-statement\"]')\n",
    "            helpful_count = w2n.word_to_num(helpful_vote_tag.text.split()[0]) if helpful_vote_tag else 0\n",
    "            result[f'Customer_{i+1}_buying_influence'] = helpful_count\n",
    "        \n",
    "\n",
    "        # Extract Top Positive and Critical Reviews (Moved outside of the above loop)\n",
    "        Parent_review_tags = soup.select('div[id^=\"viewpoint-\"]')\n",
    "        if len(Parent_review_tags) > 0: \n",
    "            ts = 'positive-review'\n",
    "            result.update(extract_specific_review(Parent_review_tags[0], 'Top_Positive', ts, soup, url))\n",
    "\n",
    "        else:\n",
    "            result.update(set_default_values('Top_Positive'))\n",
    "            \n",
    "        if len(Parent_review_tags) > 1: \n",
    "            ts = 'critical-review.a-span-last'\n",
    "            result.update(extract_specific_review(Parent_review_tags[1], 'Critical', ts, soup, url))\n",
    "\n",
    "        else:\n",
    "            result.update(set_default_values('Critical'))\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping extra parameters: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def extract_specific_review(review_tag, review_type, ts, soup, url):\n",
    "    specific_result = {}\n",
    "    \n",
    "    # Extracting ID\n",
    "    review_id = review_tag.get('id', 'None').replace('viewpoint-', '')\n",
    "    specific_result[f'{review_type}_Review_Cust_ID'] = review_id\n",
    "\n",
    "    # # Extract Customer Name and Influenced\n",
    " \n",
    "     # Corrected Extraction for Customer Name\n",
    "    customer_name_selector = 'div.a-profile-content span.a-profile-name'\n",
    "    specific_result[f'{review_type}_Review_Cust_Name'] = review_tag.select_one(customer_name_selector).text if review_tag.select_one(customer_name_selector) else 'None'\n",
    "\n",
    "    # Corrected Selector\n",
    "    influenced_selector = f'div.a-column.a-span6.view-point-review.{ts} div.a-row.a-spacing-top-small span.a-size-small.a-color-tertiary span.review-votes'\n",
    "    influenced_element = soup.select_one(influenced_selector)\n",
    "\n",
    "    if influenced_element:\n",
    "        # Directly extract the text from the found element\n",
    "        helpful_text = influenced_element.text.strip()\n",
    "        print(\"Helpful Text:\", helpful_text)  # Debugging line\n",
    "        \n",
    "        # Check if the text starts with a digit and extract the first contiguous digit sequence\n",
    "        match = re.match(r'\\d+', helpful_text)\n",
    "        if match:\n",
    "            helpful_count = int(match.group())\n",
    "        else:\n",
    "            # If the text doesn't start with a digit, try converting the first word to a number\n",
    "            helpful_count = w2n.word_to_num(helpful_text.split()[0])\n",
    "    else:\n",
    "        print(f\"Tag not found in {url}\")  # Debugging line\n",
    "        helpful_count = 0\n",
    "\n",
    "    specific_result[f'{review_type}_Review_Cust_Influenced'] = helpful_count\n",
    "\n",
    "    \n",
    " \n",
    "  # # customer_name_tags = review_tag.select('span.a-profile-name')\n",
    "    # customer_name_tags = review_tag.select('div.a-expander-content.a-expander-partial-collapse-content div.a-profile-content')\n",
    "    # specific_result[f'{review_type}_Review_Cust_Name'] = customer_name_tags[1].text if len(customer_name_tags) > 1 else 'None'\n",
    "    \n",
    "    # Extract Customer Review Comment\n",
    "    review_comment_tag = review_tag.find('div', class_='a-row a-spacing-top-mini')\n",
    "    specific_result[f'{review_type}_Review_Cust_Comment'] = review_comment_tag.text.strip() if review_comment_tag else 'None'\n",
    "    \n",
    "    # Extract Customer Review Title\n",
    "    review_title_tag = review_tag.select_one('span[data-hook=\"review-title\"]')\n",
    "    specific_result[f'{review_type}_Review_Cust_Comment_Title'] = review_title_tag.text if review_title_tag else 'None'\n",
    "    \n",
    "    # Extract the post time\n",
    "    review_tags_date = review_tag.select('div.a-expander-content.a-expander-partial-collapse-content span.a-size-base.a-color-secondary.review-date')\n",
    "    if review_tags_date:\n",
    "        post_time_text = review_tags_date[0].text.strip()\n",
    "        match = re.search(r'on (.+)$', post_time_text)\n",
    "        if match:\n",
    "            date_string = match.group(1)\n",
    "            try:\n",
    "                post_date = datetime.strptime(date_string, '%B %d, %Y')\n",
    "                specific_result[f'{review_type}_Review_Cust_Date'] = post_date.isoformat()                            \n",
    "            except ValueError as ve:\n",
    "                print(f\"Error parsing date string {date_string}: {ve}\")\n",
    "                specific_result[f'{review_type}_Review_Cust_Date'] = '-'\n",
    "        else:\n",
    "            print(\"Date not found in text:\", post_time_text)\n",
    "            specific_result[f'{review_type}_Review_Cust_Date'] = '-'\n",
    "    else:\n",
    "        print(\"Date tag not found\")\n",
    "        specific_result[f'{review_type}_Review_Cust_Date'] = None\n",
    "    \n",
    "    \n",
    "    # Extract the Star Rating\n",
    "    review_star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating-view-point\"] span.a-icon-alt')\n",
    "    star_rating = float(review_star_rating_tag.text.split()[0]) if review_star_rating_tag else 0.0\n",
    "    specific_result[f'{review_type}_Review_Cust_Star_Rating'] = star_rating\n",
    "    \n",
    "    return specific_result\n",
    "\n",
    "def set_default_values(review_type):\n",
    "    default_values = {\n",
    "        f'{review_type}_Review_Cust_ID': 'None',\n",
    "        f'{review_type}_Review_Cust_Name': 'None',\n",
    "        f'{review_type}_Review_Cust_Comment': 'None',\n",
    "        f'{review_type}_Review_Cust_Comment_Title': 'None',\n",
    "        f'{review_type}_Review_Cust_Influenced': 0,\n",
    "        f'{review_type}_Review_Cust_Star_Rating': 0.0,\n",
    "        f'{review_type}_Review_Cust_Date': None,\n",
    "    }\n",
    "    return default_values\n",
    "\n",
    "    #     return result\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error scraping extra parameters for {url}: {e}\")\n",
    "    #     traceback.print_exc()\n",
    "    # return {}\n",
    "\n",
    "def scrape_amazon(categories):\n",
    " \n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    seen_products = set()\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        for page in range(1, 2):\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#search > div.s-desktop-width-max.s-desktop-content.s-wide-grid-style-t1.s-opposite-dir.s-wide-grid-style.sg-row > div.sg-col-20-of-24.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span.rush-component.s-latency-cf-section > div.s-main-slot.s-result-list.s-search-results.sg-row > div:nth-child(1)\")))\n",
    "            except TimeoutException:\n",
    "                print(f\"Timed out waiting for elements on page {page} of category {category}.\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find all products using the given CSS selector\n",
    "            products_tags = soup.select(\"#search > div.s-desktop-width-max.s-desktop-content.s-wide-grid-style-t1.s-opposite-dir.s-wide-grid-style.sg-row > div.sg-col-20-of-24.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span.rush-component.s-latency-cf-section > div.s-main-slot.s-result-list.s-search-results.sg-row > div\")\n",
    "\n",
    "            products_list = []  # Use a different name for the list of product_dict dictionaries\n",
    "\n",
    "            for product in products_tags:\n",
    "                product_dict = {}\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Try to find item_name with the general class\n",
    "                item_name = product.find('span', class_='a-text-normal')\n",
    "\n",
    "                # If not found, try the first specific class\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If still not found, try the second specific class\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                # If item_name is found with any class, extract the text\n",
    "                if item_name:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "                else:\n",
    "                    print(f\"Failed to scrape item name in {url}\")\n",
    "                    product_dict['product'] = \"Unknown\"\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price:\n",
    "                    product_price = product_price.text.strip().replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "                    product_dict['price'] = product_price\n",
    "\n",
    "                rating_spans = product.find_all('span', attrs={\"aria-label\": True})\n",
    "                for rating_span in rating_spans:\n",
    "                    aria_label_value = rating_span.attrs[\"aria-label\"]\n",
    "                    if \"stars\" in aria_label_value:\n",
    "                        product_dict['ratings'] = aria_label_value.split(\" \")[0]\n",
    "                    else:\n",
    "                        if 'K+' in aria_label_value:\n",
    "                            product_dict['review_responders'] = aria_label_value\n",
    "                        else:\n",
    "                            try:\n",
    "                                int_value = int(aria_label_value)\n",
    "                                product_dict['review_responders'] = aria_label_value\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if item_reviews:\n",
    "                    try:\n",
    "                        reviews_text = item_reviews.text.strip()\n",
    "                        reviews_count = process_review_count(reviews_text)\n",
    "                        product_dict['reviews'] = reviews_count\n",
    "                        logging.info(f\"Successfully scraped total {product_dict['reviews']} rating for product {product_dict.get('Product_ID', 'Unknown ID')}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing review count for product {product_dict.get('Product_ID', 'Unknown ID')}: {e}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Failed to scrape total rating for product {product_dict.get('Product_ID', 'Unknown ID')}\")\n",
    "\n",
    "            \n",
    "\n",
    "                # Extract ASIN\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Construct the review URL using ASIN\n",
    "                if product_dict['Product_ID']:\n",
    "                    asin = product_dict['Product_ID']\n",
    "                    product_dict['url'] = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews\"\n",
    "                    \n",
    "                else:\n",
    "                    product_dict['url'] = \"None\"\n",
    "\n",
    "\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                if 'Product_ID' in product_dict and product_dict['Product_ID']:\n",
    "                # Create a unique identifier for the product\n",
    "                    identifier = product_dict['Product_ID']\n",
    "\n",
    "                    if identifier not in seen_products:\n",
    "                        seen_products.add(identifier) #\n",
    "                        if product_dict.get('url'):\n",
    "                            extra_params = scrape_extra_parameters(product_dict['url'], driver)\n",
    "                            product_dict.update(extra_params)\n",
    "                            products_list.append(product_dict)\n",
    "\n",
    "                        all_products.extend(products_list)\n",
    "    driver.quit()\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    categories = {\n",
    "        'Smartphones': 'https://www.amazon.com/s?k=smartphone&ref=nb_sb_noss',\n",
    "        'Laptops': 'https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss',\n",
    "        'video_games': 'https://www.amazon.com/s?k=video_games&ref=nb_sb_noss',\n",
    "        'Dresses':'https://www.amazon.com/s?k=Dresses&ref=nb_sb_noss',\n",
    "        'Shoes':'https://www.amazon.com/s?k=Shoes&ref=nb_sb_noss',\n",
    "        'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&ref=nb_sb_noss',\n",
    "    }\n",
    "\n",
    "    all_products = []\n",
    "    try:\n",
    "        all_products = json.loads(scrape_amazon(categories))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during scraping: {e}\")\n",
    "    finally:\n",
    "        with open('amazon_data_ext.json', 'w') as file:\n",
    "            json.dump(all_products, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_ext.json')\n",
    "# ---- START OF INSERTED CODE ----\n",
    "def fix_products_length(df):\n",
    "    \"\"\"Ensure each record has a length of 42 by appending None values.\"\"\"\n",
    "    max_len = df.shape[1]\n",
    "    if max_len < 42:\n",
    "        for _ in range(42 - max_len):\n",
    "            df[f'Extra_Column_{_}'] = None\n",
    "    return df\n",
    "\n",
    "# Call the function to ensure data consistency\n",
    "df = fix_products_length(df)\n",
    "# ---- END OF INSERTED CODE ----\n",
    "\n",
    "# Check if specific columns are in the DataFrame\n",
    "columns_to_check = ['Critical_Review_Cust_Influenced', 'Top_Positive_Review_Cust_Influenced']\n",
    "for column in columns_to_check:\n",
    "    if column not in df.columns:\n",
    "        logging.warning(f\"Column '{column}' not found in the DataFrame. Please check the column name in the JSON file.\")\n",
    "\n",
    "# Convert date columns to datetime objects and then to 'yyyy-mm-dd' string format\n",
    "date_columns = ['Critical_Review_Cust_Date', 'Top_Positive_Review_Cust_Date']\n",
    "for column in date_columns:\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce', format='%Y-%m-%dT%H:%M:%S')\n",
    "    df[column].fillna(pd.NaT, inplace=True)\n",
    "    df[column] = df[column].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else '1677-09-21')\n",
    "\n",
    "# Replace NaN values with 'None' in specific columns\n",
    "columns_to_replace_nan = [\n",
    "    'Critical_Review_Cust_ID', 'Critical_Review_Cust_Name', 'Critical_Review_Cust_Comment',\n",
    "    'Critical_Review_Cust_Comment_Title', 'Critical_Review_Cust_Influenced',\n",
    "    'Top_Positive_Review_Cust_ID', 'Top_Positive_Review_Cust_Name', 'Top_Positive_Review_Cust_Comment',\n",
    "    'Top_Positive_Review_Cust_Comment_Title', 'Top_Positive_Review_Cust_Influenced'\n",
    "]\n",
    "for column in columns_to_replace_nan:\n",
    "    df[column] = df[column].replace({np.nan: 'None'})\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['Product_ID'], keep='first')\n",
    "\n",
    "# Replace NaN values with 'None' in customer comment and ID columns\n",
    "for i in range(1, 6):\n",
    "    df[f'Customer_{i}_Comment'] = df[f'Customer_{i}_Comment'].replace({np.nan: 'None'})\n",
    "    df[f'Customer_{i}_ID'] = df[f'Customer_{i}_ID'].replace({np.nan: 'None'})\n",
    "\n",
    "# Define variations of NaN or missing values and replace in customer comment columns\n",
    "nan_variants = [np.nan, 'NaN', 'nan', 'None', 'none', 'N/A', 'n/a', 'NA', 'na', 'null', '']\n",
    "for i in range(1, 6):\n",
    "    col_name = f'Customer_{i}_Comment'\n",
    "    df[col_name] = df[col_name].astype(str).replace(nan_variants, 'None')\n",
    "\n",
    "# Drop rows where specific columns have undesired values\n",
    "df.dropna(subset=['price'], inplace=True)\n",
    "df.drop(df.index[df['Customer_1_ID'] == 'None'], inplace=True)\n",
    "df.drop(df.index[df['reviews'] == '0'], inplace=True)\n",
    "\n",
    "# Update the 'Critical_Review_Cust_Influenced' and 'Top_Positive_Review_Cust_Influenced' columns\n",
    "for column in ['Critical_Review_Cust_Influenced', 'Top_Positive_Review_Cust_Influenced']:\n",
    "    df[column] = df[column].replace({'\"NaN\"': 0.0, 'NaN': 0.0})\n",
    "\n",
    "# Drop the 'review_responders' column if it exists\n",
    "if 'review_responders' in df.columns:\n",
    "    df.drop(columns=['review_responders'], inplace=True)\n",
    "\n",
    "# Clean other columns\n",
    "df['price'] = df['price'].apply(lambda value: float(value) if isinstance(value, (int, float)) else 0.0)\n",
    "df['ratings'] = df['ratings'].apply(lambda x: float(x) if pd.notna(x) else None)\n",
    "df['reviews'] = df['reviews'].str.replace('(', '').str.replace(')', '').replace(nan_variants, 0).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\",\n",
    "    client_encoding='utf8'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Modify the CREATE TABLE query to include additional columns\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_data_ext;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data_ext (\n",
    "    Product_ID TEXT NOT NULL,\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NULL,\n",
    "    ratings NUMERIC NULL,\n",
    "    reviews INTEGER NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    url TEXT NOT NULL,\n",
    "    Top_Positive_Review_Cust_ID TEXT,\n",
    "    Top_Positive_Review_Cust_Name TEXT,\n",
    "    Top_Positive_Review_Cust_Date DATE,\n",
    "    Top_Positive_Review_Cust_Comment TEXT,\n",
    "    Top_Positive_Review_Cust_Comment_Title TEXT,\n",
    "    Top_Positive_Review_Cust_Influenced INTEGER,\n",
    "    Top_Positive_Review_Cust_Star_Rating NUMERIC,\n",
    "    Critical_Review_Cust_ID TEXT,\n",
    "    Critical_Review_Cust_Name TEXT,\n",
    "    Critical_Review_Cust_Date DATE,\n",
    "    Critical_Review_Cust_Comment TEXT,\n",
    "    Critical_Review_Cust_Comment_Title TEXT,\n",
    "    Critical_Review_Cust_Influenced INTEGER,\n",
    "    Critical_Review_Cust_Star_Rating NUMERIC,\n",
    "    \"\"\" + \",\\n    \".join([f\"Customer_{i}_ID TEXT, Customer_{i}_Star_Rating NUMERIC, Customer_{i}_Comment TEXT, Customer_{i}_buying_influence INTEGER\" for i in range(1, 6)]) + \"\"\"\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "def clean_format_data(row):\n",
    "    # Extract values directly, as they are already cleaned\n",
    "    ratings = row['ratings']\n",
    "    price = row['price']\n",
    "    reviews = row['reviews']\n",
    "    product_id = row['Product_ID']\n",
    "    product = row['product']\n",
    "    category = row['category']\n",
    "    url = row['url']\n",
    "      \n",
    "    critical_review_id = row['Critical_Review_Cust_ID'] if row['Critical_Review_Cust_ID'] != 'None' else None\n",
    "    critical_review_cust_name = row['Critical_Review_Cust_Name'] if row['Critical_Review_Cust_Name'] != 'None' else None\n",
    "    critical_review_cust_comment = row['Critical_Review_Cust_Comment']\n",
    "    if critical_review_cust_comment in nan_variants or critical_review_cust_comment == 'None':\n",
    "        critical_review_cust_comment = 'Unavailable'\n",
    "    \n",
    "    critical_review_cust_comment_title = row['Critical_Review_Cust_Comment_Title'] if row['Critical_Review_Cust_Comment_Title'] != 'None' else None\n",
    "    critical_review_cust_influenced = row['Critical_Review_Cust_Influenced'] if row['Critical_Review_Cust_Influenced'] != 'None' else 0  # Correctly handle NaN values\n",
    "    critical_review_star_rating = row['Critical_Review_Cust_Star_Rating'] if pd.notna(row['Critical_Review_Cust_Star_Rating']) else 0.0\n",
    "    critical_review_cust_date = row['Critical_Review_Cust_Date'] if row['Critical_Review_Cust_Date'] != 'None' else '0001-01-01'  # Correctly handle NaN values\n",
    "\n",
    "    top_positive_review_id = row['Top_Positive_Review_Cust_ID'] if row['Top_Positive_Review_Cust_ID'] != 'None' else None\n",
    "    top_positive_review_cust_name = row['Top_Positive_Review_Cust_Name'] if row['Top_Positive_Review_Cust_Name'] != 'None' else None\n",
    "    top_positive_review_cust_comment = row['Critical_Review_Cust_Comment']\n",
    "    if top_positive_review_cust_comment in nan_variants or top_positive_review_cust_comment == 'None':\n",
    "        top_positive_review_cust_comment = None    \n",
    "    top_positive_review_cust_comment_title = row['Top_Positive_Review_Cust_Comment_Title'] if row['Top_Positive_Review_Cust_Comment_Title'] != 'None' else None\n",
    "    top_positive_review_cust_influenced = row['Top_Positive_Review_Cust_Influenced'] if row['Top_Positive_Review_Cust_Influenced'] != 'None' else 0  # Correctly handle NaN values\n",
    "    top_positive_review_star_rating = row['Top_Positive_Review_Cust_Star_Rating'] if pd.notna(row['Top_Positive_Review_Cust_Star_Rating']) else 0.0\n",
    "    top_positive_review_cust_date = row['Top_Positive_Review_Cust_Date'] if row['Top_Positive_Review_Cust_Date'] != 'None' else '0001-01-01'  # Correctly handle NaN values\n",
    "    \n",
    "    top_positive_date = row['Top_Positive_Review_Cust_Date']\n",
    "    critical_review_date = row['Critical_Review_Cust_Date']\n",
    "\n",
    "    def format_date(date_value):\n",
    "        if isinstance(date_value, str) and re.match(r'\\d{4}-\\d{2}-\\d{2}', date_value):\n",
    "            return date_value\n",
    "        else:\n",
    "            return '0001-01-01'  # Default value for invalid date formats or non-string values\n",
    "\n",
    "    top_positive_review_cust_date = format_date(top_positive_date)\n",
    "    critical_review_cust_date = format_date(critical_review_date)\n",
    "\n",
    "\n",
    "\n",
    "    # Handle additional customer information\n",
    "    customer_data = []\n",
    "    for i in range(1, 6):\n",
    "        customer_id = row[f'Customer_{i}_ID'] if row[f'Customer_{i}_ID'] != 'None' else \"Unavailable\"\n",
    "        star_rating = row[f'Customer_{i}_Star_Rating'] if pd.notna(row[f'Customer_{i}_Star_Rating']) else 0.0\n",
    "        comment = row[f'Customer_{i}_Comment'] if row[f'Customer_{i}_Comment'] != 'None' else \"Unavailable\"\n",
    "        buying_influence = row[f'Customer_{i}_buying_influence'] if pd.notna(row[f'Customer_{i}_buying_influence']) else 0\n",
    "        customer_data.extend([customer_id, star_rating, comment, buying_influence])\n",
    "\n",
    "    # Construct the return tuple\n",
    "    result_tuple = (product_id, product, price, ratings, reviews, category, url, \n",
    "                   top_positive_review_id, top_positive_review_cust_name, top_positive_review_cust_date, \n",
    "                   top_positive_review_cust_comment, top_positive_review_cust_comment_title, \n",
    "                   top_positive_review_cust_influenced, top_positive_review_star_rating, \n",
    "                   critical_review_id, critical_review_cust_name, critical_review_cust_date, critical_review_cust_comment, \n",
    "                   critical_review_cust_comment_title, critical_review_cust_influenced, \n",
    "                   critical_review_star_rating, *customer_data)\n",
    "    \n",
    "    if not result_tuple:\n",
    "        logging.error(f\"Failed to construct tuple for row: {row}\")\n",
    "        return None\n",
    "    \n",
    "    return result_tuple\n",
    "\n",
    "# Check for the presence of the column `Customer_{i}_buying_influence` in the DataFrame\n",
    "for i in range(1, 6):\n",
    "    if f'Customer_{i}_buying_influence' not in df.columns:\n",
    "        logging.error(f\"Column 'Customer_{i}_buying_influence' not found in the DataFrame.\")\n",
    "\n",
    "\n",
    "# Define the INSERT query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO amazon_data_ext (\n",
    "    Product_ID, product, price, ratings, reviews, category, url,\n",
    "    Top_Positive_Review_Cust_ID, Top_Positive_Review_Cust_Name, Top_Positive_Review_Cust_Date, Top_Positive_Review_Cust_Comment, Top_Positive_Review_Cust_Comment_Title, Top_Positive_Review_Cust_Influenced, Top_Positive_Review_Cust_Star_Rating, Critical_Review_Cust_ID, Critical_Review_Cust_Name, Critical_Review_Cust_Date, Critical_Review_Cust_Comment, Critical_Review_Cust_Comment_Title, Critical_Review_Cust_Influenced, Critical_Review_Cust_Star_Rating,\n",
    "    \"\"\" + \", \".join([f\"Customer_{i}_ID, Customer_{i}_Star_Rating, Customer_{i}_Comment, Customer_{i}_buying_influence\" for i in range(1, 6)]) + \"\"\"\n",
    ") VALUES (\"\"\" + \", \".join([\"%s\"] * (21 + 20)) + \")\"\n",
    "\n",
    "\n",
    "# Count the number of placeholders in the SQL query\n",
    "num_placeholders = insert_query.count('%s')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tuple_values = clean_format_data(row)\n",
    "    if not tuple_values:\n",
    "        logging.warning(f\"Skipping row at index {index} due to errors in data processing.\")\n",
    "        continue\n",
    "    num_tuple_values = len(tuple_values)\n",
    "    \n",
    "    # Check for mismatch between placeholders and tuple values\n",
    "    if num_placeholders != num_tuple_values:\n",
    "        logging.error(f\"Mismatch at index {index}! Number of placeholders: {num_placeholders}, Number of tuple values: {num_tuple_values}\")\n",
    "        logging.error(f\"Tuple values: {tuple_values}\")\n",
    "        \n",
    "        # Expected columns based on the INSERT query\n",
    "        expected_columns = [\n",
    "            \"Product_ID\", \"product\", \"price\", \"ratings\", \"reviews\", \"category\", \"url\",\n",
    "            \"Top_Positive_Review_Cust_ID\", \"Top_Positive_Review_Cust_Name\", \"Top_Positive_Review_Cust_Date\", \"Top_Positive_Review_Cust_Comment\", \"Top_Positive_Review_Cust_Comment_Title\", \"Top_Positive_Review_Cust_Influenced\",\n",
    "            \"Top_Positive_Review_Cust_Star_Rating\", \"Critical_Review_Cust_ID\", \"Critical_Review_Cust_Name\", \"Critical_Review_Cust_Date\", \"Critical_Review_Cust_Comment\", \"Critical_Review_Cust_Comment_Title\",\n",
    "            \"Critical_Review_Cust_Influenced\", \"Critical_Review_Cust_Star_Rating\"\n",
    "        ] + [f\"Customer_{i}_ID\" for i in range(1, 6)] + [f\"Customer_{i}_Star_Rating\" for i in range(1, 6)] + [f\"Customer_{i}_Comment\" for i in range(1, 6)] + [f\"Customer_{i}_buying_influence\" for i in range(1, 6)]\n",
    "\n",
    "\n",
    "\n",
    "        # In the section where you're logging the mismatch error, add this:\n",
    "        for col, val in zip(expected_columns, tuple_values):\n",
    "            print(f\"{col}: {val}\")\n",
    "\n",
    "        \n",
    "        continue  # Skip this iteration\n",
    "\n",
    "\n",
    "    try:\n",
    "        cur.execute(insert_query, tuple_values)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting row at index {index}: {e}\")\n",
    "        logging.debug(f\"Row data: {row}\")\n",
    "        conn.rollback()\n",
    "\n",
    "        \n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "# Rename the columns in the DataFrame\n",
    "df.rename(columns={'ratings': 'star_ratings', 'reviews': 'total_ratings', 'price': 'price_dollars'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file with updated column names\n",
    "df.to_csv('amazon_data_ext.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
