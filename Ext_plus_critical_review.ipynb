{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:WDM:====== WebDriver manager ======\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:There is no [win64] edgedriver \"117.0.2045.60\" for browser edge \"117.0.2045\" in cache\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:About to download new driver from https://msedgedriver.azureedge.net/117.0.2045.60/edgedriver_win64.zip\n",
      "INFO:WDM:Driver downloading response is 200\n",
      "INFO:WDM:Get LATEST edgedriver version for Edge 117.0.2045\n",
      "INFO:WDM:Driver has been saved in cache [C:\\Users\\Kasim\\.wdm\\drivers\\edgedriver\\win64\\117.0.2045.60]\n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "INFO:root:Successfully scraped total 65 rating for product B0CC2GD4D9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 654 rating for product B0C3JB53RQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 7 people found this helpful\n",
      "Helpful Text: 10 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 79 rating for product B0BY3PGDZR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0C3JB53RQ/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n",
      "Helpful Text: One person found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 307 rating for product B0BWSG8VDK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 14 people found this helpful\n",
      "Helpful Text: 3 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 495 rating for product B0B2D77YB8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 12 people found this helpful\n",
      "Helpful Text: 12 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 45 rating for product B0C3RNRB8W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to scrape total rating for product B0CHN3YKW8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0C3RNRB8W/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n",
      "Helpful Text: 19 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 154 rating for product B0CGMQ1R1F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException: Could not find reviews for https://www.amazon.com/product-reviews/B0CHN3YKW8/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 36480 rating for product B0BS4BP8FB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 21 people found this helpful\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0CGMQ1R1F/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 160 rating for product B0CGD8W4Y3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 460 people found this helpful\n",
      "Helpful Text: 92 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 458 rating for product B0B5HQTHYZ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0CGD8W4Y3/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n",
      "Helpful Text: One person found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 83 rating for product B0BY34X9J4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 6 people found this helpful\n",
      "Helpful Text: 3 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 3 rating for product B0CBD6GJN7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 25 people found this helpful\n",
      "Helpful Text: 7 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 491 rating for product B0B273H99Y\n",
      "WARNING:root:Failed to scrape total rating for product B0CJ541S55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 2 people found this helpful\n",
      "Helpful Text: 2 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 886 rating for product B0CHFGKW3W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutException: Could not find reviews for https://www.amazon.com/product-reviews/B0CJ541S55/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 43 rating for product B0C6YRCFM2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: 4 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 16 rating for product B0B6PNH9V2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0C6YRCFM2/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n",
      "Helpful Text: 4 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 36 rating for product B0CB5NZRZM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: One person found this helpful\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0B6PNH9V2/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 318 rating for product B0C33KJV5N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag not found in https://www.amazon.com/product-reviews/B0CB5NZRZM/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0CB5NZRZM/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 1160 rating for product B0CBXLWGK9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: 11 people found this helpful\n",
      "Tag not found in https://www.amazon.com/product-reviews/B0C33KJV5N/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 11 rating for product B0BN5KKKNY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpful Text: One person found this helpful\n",
      "Helpful Text: 18 people found this helpful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully scraped total 405 rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n",
      "WARNING:root:Failed to scrape total rating for product \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n",
      "Failed to scrape item name in https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss&page=1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import traceback\n",
    "import logging\n",
    "import re  # Make sure to include this import\n",
    "from word2number import w2n\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_review_count(text):\n",
    "    text = text.strip().replace(',', '')\n",
    "    if 'K+' in text:\n",
    "        return str(int(float(text.replace('(', '').replace(')', '').replace('K+', '').strip()) * 1000))\n",
    "    return text\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    try:\n",
    "        driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Failed to install Edge Chromium driver.\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "\n",
    "def scrape_extra_parameters(url: str, driver: webdriver.Edge) -> dict:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-hook='review']\")))\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"TimeoutException: Could not find reviews for {url}\")\n",
    "            return {}\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract the general reviews\n",
    "        reviews_tags = soup.find_all('div', attrs={'data-hook': 'review'})\n",
    "\n",
    "        result = {}\n",
    "        for i, review_tag in enumerate(reviews_tags[:5]):\n",
    "            result[f'Customer_{i + 1}_ID'] = review_tag.attrs.get('id', 'None')\n",
    "            \n",
    "            # Extract the Star Rating\n",
    "            star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating\"] span.a-icon-alt')\n",
    "            star_rating = float(star_rating_tag.text.split()[0]) if star_rating_tag else 0.0\n",
    "            result[f'Customer_{i+1}_Star_Rating'] = star_rating\n",
    "            \n",
    "            # Extract the Comment Title\n",
    "            comment_title_tag = review_tag.select_one('a[data-hook=\"review-title\"]')\n",
    "            # Inside the for loop, after extracting the comment title:\n",
    "            if comment_title_tag:\n",
    "                actual_comment_title = comment_title_tag.text.strip()\n",
    "            else:\n",
    "                # Handle alternate structure\n",
    "                comment_title_tag = review_tag.select_one('span.cr-original-review-content')\n",
    "                actual_comment_title = comment_title_tag.text.strip() if comment_title_tag else 'NaN'\n",
    "\n",
    "            # Remove the pattern \"k out of 5 stars\\n\" from the comment\n",
    "            actual_comment_title = re.sub(r'\\d+(\\.\\d+)? out of 5 stars\\n', '', actual_comment_title)\n",
    "\n",
    "            result[f'Customer_{i+1}_Comment'] = actual_comment_title\n",
    "\n",
    "            # Extract the Number of people who found the review helpful\n",
    "            helpful_vote_tag = review_tag.select_one('span[data-hook=\"helpful-vote-statement\"]')\n",
    "            helpful_count = w2n.word_to_num(helpful_vote_tag.text.split()[0]) if helpful_vote_tag else 0\n",
    "            result[f'Customer_{i+1}_buying_influence'] = helpful_count\n",
    "\n",
    "\n",
    "            ####test start\n",
    "\n",
    "            # Extract the post time\n",
    "            review_tags_date_selector = f'#customer_review-{result[f\"Customer_{i + 1}_ID\"]} span.a-size-base.a-color-secondary.review-date'\n",
    "            review_tags_date = review_tag.select(review_tags_date_selector)\n",
    "\n",
    "            if review_tags_date:\n",
    "                post_time_text = review_tags_date[0].text.strip()\n",
    "                match = re.search(r'on (.+)$', post_time_text)\n",
    "                if match:\n",
    "                    date_string = match.group(1)\n",
    "                    try:\n",
    "                        post_date = datetime.strptime(date_string, '%B %d, %Y')\n",
    "                        result[f'Review_Cust_Date_{i+1}'] = post_date.isoformat()\n",
    "                    except ValueError as ve:\n",
    "                        print(f\"Error parsing date string {date_string}: {ve}\")\n",
    "                        result[f'Review_Cust_Date_{i+1}'] = '-'\n",
    "                else:\n",
    "                    print(\"Date not found in text:\", post_time_text)\n",
    "                    result[f'Review_Cust_Date_{i+1}'] = '-'\n",
    "            else:\n",
    "                print(\"Date tag not found\")\n",
    "                result[f'Review_Cust_Date_{i+1}'] = None\n",
    "\n",
    "           \n",
    "\n",
    "            # Extract the location\n",
    "            review_tags_location_selector = f'#customer_review-{result[f\"Customer_{i + 1}_ID\"]} span.a-size-base.a-color-secondary.review-date'\n",
    "            review_tags_location = review_tag.select(review_tags_location_selector)\n",
    "\n",
    "            if review_tags_location:\n",
    "                post_location_text = review_tags_location[0].text.strip()\n",
    "                match = re.search(r\"Reviewed in the (.+?) on\", post_location_text)\n",
    "                if match:\n",
    "                    country = match.group(1)\n",
    "                    result[f'Review_Cust_Location_{i+1}'] = country                 \n",
    "\n",
    "                else:\n",
    "                    print(\"Unknown location\")\n",
    "                    result[f'Review_Cust_Location_{i+1}'] = 'Unknown location'\n",
    "            else:\n",
    "                print(\"location tag not found\")\n",
    "                result[f'Review_Cust_Location_{i+1}'] = None\n",
    "\n",
    "            ####test end\n",
    "\n",
    "\n",
    "        # Extract Top Positive and Critical Reviews (Moved outside of the above loop)\n",
    "        Parent_review_tags = soup.select('div[id^=\"viewpoint-\"]')\n",
    "        if len(Parent_review_tags) > 0: \n",
    "            ts = 'positive-review'\n",
    "            result.update(extract_specific_review(Parent_review_tags[0], 'Top_Positive', ts, soup, url))\n",
    "\n",
    "        else:\n",
    "            result.update(set_default_values('Top_Positive'))\n",
    "            \n",
    "        if len(Parent_review_tags) > 1: \n",
    "            ts = 'critical-review.a-span-last'\n",
    "            result.update(extract_specific_review(Parent_review_tags[1], 'Critical', ts, soup, url))\n",
    "\n",
    "        else:\n",
    "            result.update(set_default_values('Critical'))\n",
    "            \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping extra parameters: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def extract_specific_review(review_tag, review_type, ts, soup, url):\n",
    "    specific_result = {}\n",
    "    \n",
    "    # Extracting ID\n",
    "    review_id = review_tag.get('id', 'None').replace('viewpoint-', '')\n",
    "    specific_result[f'{review_type}_Review_Cust_ID'] = review_id\n",
    "\n",
    "    # # Extract Customer Name and Influenced\n",
    " \n",
    "     # Corrected Extraction for Customer Name\n",
    "    customer_name_selector = 'div.a-profile-content span.a-profile-name'\n",
    "    specific_result[f'{review_type}_Review_Cust_Name'] = review_tag.select_one(customer_name_selector).text if review_tag.select_one(customer_name_selector) else 'None'\n",
    "\n",
    "    # Corrected Selector\n",
    "    influenced_selector = f'div.a-column.a-span6.view-point-review.{ts} div.a-row.a-spacing-top-small span.a-size-small.a-color-tertiary span.review-votes'\n",
    "    influenced_element = soup.select_one(influenced_selector)\n",
    "\n",
    "    if influenced_element:\n",
    "        # Directly extract the text from the found element\n",
    "        helpful_text = influenced_element.text.strip()\n",
    "        print(\"Helpful Text:\", helpful_text)  # Debugging line\n",
    "        \n",
    "        # Check if the text starts with a digit and extract the first contiguous digit sequence\n",
    "        match = re.match(r'\\d+', helpful_text)\n",
    "        if match:\n",
    "            helpful_count = int(match.group())\n",
    "        else:\n",
    "            # If the text doesn't start with a digit, try converting the first word to a number\n",
    "            helpful_count = w2n.word_to_num(helpful_text.split()[0])\n",
    "    else:\n",
    "        print(f\"Tag not found in {url}\")  # Debugging line\n",
    "        helpful_count = 0\n",
    "\n",
    "    specific_result[f'{review_type}_Review_Cust_Influenced'] = helpful_count\n",
    "\n",
    "    \n",
    "    # Extract Customer Review Comment\n",
    "    review_comment_tag = review_tag.find('div', class_='a-row a-spacing-top-mini')\n",
    "    specific_result[f'{review_type}_Review_Cust_Comment'] = review_comment_tag.text.strip() if review_comment_tag else 'None'\n",
    "    \n",
    "    # Extract Customer Review Title\n",
    "    review_title_tag = review_tag.select_one('span[data-hook=\"review-title\"]')\n",
    "    specific_result[f'{review_type}_Review_Cust_Comment_Title'] = review_title_tag.text if review_title_tag else 'None'\n",
    "    \n",
    "    # Extract the post time\n",
    "    review_tags_date = review_tag.select('div.a-expander-content.a-expander-partial-collapse-content span.a-size-base.a-color-secondary.review-date')\n",
    "    if review_tags_date:\n",
    "        post_time_text = review_tags_date[0].text.strip()\n",
    "        match = re.search(r'on (.+)$', post_time_text)\n",
    "        if match:\n",
    "            date_string = match.group(1)\n",
    "            try:\n",
    "                post_date = datetime.strptime(date_string, '%B %d, %Y')\n",
    "                specific_result[f'{review_type}_Review_Cust_Date'] = post_date.isoformat()                            \n",
    "            except ValueError as ve:\n",
    "                print(f\"Error parsing date string {date_string}: {ve}\")\n",
    "                specific_result[f'{review_type}_Review_Cust_Date'] = '-'\n",
    "        else:\n",
    "            print(\"Date not found in text:\", post_time_text)\n",
    "            specific_result[f'{review_type}_Review_Cust_Date'] = '-'\n",
    "    else:\n",
    "        print(\"Date tag not found\")\n",
    "        specific_result[f'{review_type}_Review_Cust_Date'] = None\n",
    "    \n",
    "    \n",
    "    # Extract the Star Rating\n",
    "    review_star_rating_tag = review_tag.select_one('i[data-hook=\"review-star-rating-view-point\"] span.a-icon-alt')\n",
    "    star_rating = float(review_star_rating_tag.text.split()[0]) if review_star_rating_tag else 0.0\n",
    "    specific_result[f'{review_type}_Review_Cust_Star_Rating'] = star_rating\n",
    "    \n",
    "    return specific_result\n",
    "\n",
    "def set_default_values(review_type):\n",
    "    default_values = {\n",
    "        f'{review_type}_Review_Cust_ID': 'None',\n",
    "        f'{review_type}_Review_Cust_Name': 'None',\n",
    "        f'{review_type}_Review_Cust_Comment': 'None',\n",
    "        f'{review_type}_Review_Cust_Comment_Title': 'None',\n",
    "        f'{review_type}_Review_Cust_Influenced': 0,\n",
    "        f'{review_type}_Review_Cust_Star_Rating': 0.0,\n",
    "        f'{review_type}_Review_Cust_Date': None,\n",
    "    }\n",
    "    return default_values\n",
    "\n",
    "    #     return result\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error scraping extra parameters for {url}: {e}\")\n",
    "    #     traceback.print_exc()\n",
    "    # return {}\n",
    "\n",
    "def scrape_amazon(categories):\n",
    " \n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    seen_products = set()\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        for page in range(1, 2):\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#search > div.s-desktop-width-max.s-desktop-content.s-wide-grid-style-t1.s-opposite-dir.s-wide-grid-style.sg-row > div.sg-col-20-of-24.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span.rush-component.s-latency-cf-section > div.s-main-slot.s-result-list.s-search-results.sg-row > div:nth-child(1)\")))\n",
    "            except TimeoutException:\n",
    "                print(f\"Timed out waiting for elements on page {page} of category {category}.\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(random.uniform(3.0, 6.0))\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find all products using the given CSS selector\n",
    "            products_tags = soup.select(\"#search > div.s-desktop-width-max.s-desktop-content.s-wide-grid-style-t1.s-opposite-dir.s-wide-grid-style.sg-row > div.sg-col-20-of-24.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span.rush-component.s-latency-cf-section > div.s-main-slot.s-result-list.s-search-results.sg-row > div\")\n",
    "\n",
    "            products_list = []  # Use a different name for the list of product_dict dictionaries\n",
    "\n",
    "            for product in products_tags:\n",
    "                product_dict = {}\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Try to find item_name with the general class\n",
    "                item_name = product.find('span', class_='a-text-normal')\n",
    "\n",
    "                # If not found, try the first specific class\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If still not found, try the second specific class\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                # If item_name is found with any class, extract the text\n",
    "                if item_name:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "                else:\n",
    "                    print(f\"Failed to scrape item name in {url}\")\n",
    "                    product_dict['product'] = \"Unknown\"\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price:\n",
    "                    product_price = product_price.text.strip().replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "                    product_dict['price'] = product_price\n",
    "\n",
    "                rating_spans = product.find_all('span', attrs={\"aria-label\": True})\n",
    "                for rating_span in rating_spans:\n",
    "                    aria_label_value = rating_span.attrs[\"aria-label\"]\n",
    "                    if \"stars\" in aria_label_value:\n",
    "                        product_dict['ratings'] = aria_label_value.split(\" \")[0]\n",
    "                    else:\n",
    "                        if 'K+' in aria_label_value:\n",
    "                            product_dict['review_responders'] = aria_label_value\n",
    "                        else:\n",
    "                            try:\n",
    "                                int_value = int(aria_label_value)\n",
    "                                product_dict['review_responders'] = aria_label_value\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if item_reviews:\n",
    "                    try:\n",
    "                        reviews_text = item_reviews.text.strip()\n",
    "                        reviews_count = process_review_count(reviews_text)\n",
    "                        product_dict['reviews'] = reviews_count\n",
    "                        logging.info(f\"Successfully scraped total {product_dict['reviews']} rating for product {product_dict.get('Product_ID', 'Unknown ID')}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing review count for product {product_dict.get('Product_ID', 'Unknown ID')}: {e}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Failed to scrape total rating for product {product_dict.get('Product_ID', 'Unknown ID')}\")\n",
    "\n",
    "            \n",
    "\n",
    "                # Extract ASIN\n",
    "                product_dict['Product_ID'] = product.attrs.get('data-asin', None)\n",
    "\n",
    "                # Construct the review URL using ASIN\n",
    "                if product_dict['Product_ID']:\n",
    "                    asin = product_dict['Product_ID']\n",
    "                    product_dict['url'] = f\"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_top?ie=UTF8&reviewerType=all_reviews&sortBy=recent\"\n",
    "                else:\n",
    "                    product_dict['url'] = \"None\"\n",
    "\n",
    "\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                if 'Product_ID' in product_dict and product_dict['Product_ID']:\n",
    "                # Create a unique identifier for the product\n",
    "                    identifier = product_dict['Product_ID']\n",
    "\n",
    "                    if identifier not in seen_products:\n",
    "                        seen_products.add(identifier) #\n",
    "                        if product_dict.get('url'):\n",
    "                            extra_params = scrape_extra_parameters(product_dict['url'], driver)\n",
    "                            product_dict.update(extra_params)\n",
    "                            products_list.append(product_dict)\n",
    "\n",
    "                        all_products.extend(products_list)\n",
    "    driver.quit()\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    categories = {\n",
    "        # 'Smartphones': 'https://www.amazon.com/s?k=smartphone&ref=nb_sb_noss',\n",
    "        'Laptops': 'https://www.amazon.com/s?k=Laptops&ref=nb_sb_noss',\n",
    "        # 'video_games': 'https://www.amazon.com/s?k=video_games&ref=nb_sb_noss',\n",
    "        # 'Dresses':'https://www.amazon.com/s?k=Dresses&ref=nb_sb_noss',\n",
    "        # 'Shoes':'https://www.amazon.com/s?k=Shoes&ref=nb_sb_noss',\n",
    "        # 'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&ref=nb_sb_noss',\n",
    "    }\n",
    "\n",
    "    all_products = []\n",
    "    try:\n",
    "        all_products = json.loads(scrape_amazon(categories))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during scraping: {e}\")\n",
    "    finally:\n",
    "        with open('amazon_data_ext.json', 'w') as file:\n",
    "            json.dump(all_products, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Review_Cust_Date_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review_Cust_Date_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kasim\\OneDrive\\Desktop\\ebejale_Desk_Folder\\CICD\\9.  future tasks.cloud computing, capstone, and IT projects\\Internship_Project\\Ext_plus_critical_review.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=222'>223</a>\u001b[0m num_placeholders \u001b[39m=\u001b[39m insert_query\u001b[39m.\u001b[39mcount(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m     tuple_values \u001b[39m=\u001b[39m clean_format_data(row)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tuple_values:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipping row at index \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m due to errors in data processing.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Kasim\\OneDrive\\Desktop\\ebejale_Desk_Folder\\CICD\\9.  future tasks.cloud computing, capstone, and IT projects\\Internship_Project\\Ext_plus_critical_review.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m comment \u001b[39m=\u001b[39m replace_with_unavailable(row[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCustomer_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_Comment\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m buying_influence \u001b[39m=\u001b[39m row[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCustomer_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_buying_influence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m pd\u001b[39m.\u001b[39mnotna(row[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCustomer_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_buying_influence\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m review_cust_date \u001b[39m=\u001b[39m format_date(row[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mReview_Cust_Date_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m review_cust_location \u001b[39m=\u001b[39m replace_with_unavailable(row[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReview_Cust_Location_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship_Project/Ext_plus_critical_review.ipynb#W1sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m customer_data\u001b[39m.\u001b[39mextend([customer_id, star_rating, comment, buying_influence, review_cust_date, review_cust_location])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review_Cust_Date_1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_ext.json')\n",
    "# ---- START OF INSERTED CODE ----\n",
    "def fix_products_length(df):\n",
    "    \"\"\"Ensure each record has a length of 42 by appending None values.\"\"\"\n",
    "    max_len = df.shape[1]\n",
    "    if max_len < 42:\n",
    "        for _ in range(42 - max_len):\n",
    "            df[f'Extra_Column_{_}'] = None\n",
    "    return df\n",
    "\n",
    "# Call the function to ensure data consistency\n",
    "df = fix_products_length(df)\n",
    "# ---- END OF INSERTED CODE ----\n",
    "\n",
    "# Check if specific columns are in the DataFrame\n",
    "columns_to_check = ['Critical_Review_Cust_Influenced', 'Top_Positive_Review_Cust_Influenced']\n",
    "for column in columns_to_check:\n",
    "    if column not in df.columns:\n",
    "        logging.warning(f\"Column '{column}' not found in the DataFrame. Please check the column name in the JSON file.\")\n",
    "\n",
    "# Convert date columns to datetime objects and then to 'yyyy-mm-dd' string format\n",
    "date_columns = ['Critical_Review_Cust_Date', 'Top_Positive_Review_Cust_Date']\n",
    "for column in date_columns:\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce', format='%Y-%m-%dT%H:%M:%S')\n",
    "    df[column].fillna(pd.NaT, inplace=True)\n",
    "    df[column] = df[column].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else '1677-09-21')\n",
    "\n",
    "# Replace NaN values with 'None' in specific columns\n",
    "columns_to_replace_nan = [\n",
    "    'Critical_Review_Cust_ID', 'Critical_Review_Cust_Name', 'Critical_Review_Cust_Comment',\n",
    "    'Critical_Review_Cust_Comment_Title', 'Critical_Review_Cust_Influenced',\n",
    "    'Top_Positive_Review_Cust_ID', 'Top_Positive_Review_Cust_Name', 'Top_Positive_Review_Cust_Comment',\n",
    "    'Top_Positive_Review_Cust_Comment_Title', 'Top_Positive_Review_Cust_Influenced'\n",
    "]\n",
    "for column in columns_to_replace_nan:\n",
    "    df[column] = df[column].replace({np.nan: 'None'})\n",
    "\n",
    "# Remove any duplicates that may have been created due to URL changes\n",
    "df = df.drop_duplicates(subset=['Product_ID'], keep='first')\n",
    "\n",
    "# Replace NaN values with 'None' in customer comment and ID columns\n",
    "for i in range(1, 6):\n",
    "    df[f'Customer_{i}_Comment'] = df[f'Customer_{i}_Comment'].replace({np.nan: 'None'})\n",
    "    df[f'Customer_{i}_ID'] = df[f'Customer_{i}_ID'].replace({np.nan: 'None'})\n",
    "\n",
    "# Define variations of NaN or missing values and replace in customer comment columns\n",
    "nan_variants = [np.nan, 'NaN', 'nan', 'None', 'none', 'N/A', 'n/a', 'NA', 'na', 'null', '']\n",
    "for i in range(1, 6):\n",
    "    col_name = f'Customer_{i}_Comment'\n",
    "    df[col_name] = df[col_name].astype(str).replace(nan_variants, 'None')\n",
    "\n",
    "# Drop rows where specific columns have undesired values\n",
    "df.dropna(subset=['price'], inplace=True)\n",
    "df.drop(df.index[df['Customer_1_ID'] == 'None'], inplace=True)\n",
    "df.drop(df.index[df['reviews'] == '0'], inplace=True)\n",
    "\n",
    "# Update the 'Critical_Review_Cust_Influenced' and 'Top_Positive_Review_Cust_Influenced' columns\n",
    "for column in ['Critical_Review_Cust_Influenced', 'Top_Positive_Review_Cust_Influenced']:\n",
    "    df[column] = df[column].replace({'\"NaN\"': 0.0, 'NaN': 0.0})\n",
    "\n",
    "# Drop the 'review_responders' column if it exists\n",
    "if 'review_responders' in df.columns:\n",
    "    df.drop(columns=['review_responders'], inplace=True)\n",
    "\n",
    "# Clean other columns\n",
    "df['price'] = df['price'].apply(lambda value: float(value) if isinstance(value, (int, float)) else 0.0)\n",
    "df['ratings'] = df['ratings'].apply(lambda x: float(x) if pd.notna(x) else None)\n",
    "df['reviews'] = df['reviews'].replace('(', '').replace(')', '').replace(nan_variants, 0).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\",\n",
    "    client_encoding='utf8'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Modify the CREATE TABLE query to include additional columns\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_data_ext;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data_ext (\n",
    "    Product_ID TEXT NOT NULL,\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NULL,\n",
    "    ratings NUMERIC NULL,\n",
    "    reviews INTEGER NOT NULL,\n",
    "    category TEXT NOT NULL,\n",
    "    url TEXT NOT NULL,\n",
    "    Top_Positive_Review_Cust_ID TEXT,\n",
    "    Top_Positive_Review_Cust_Name TEXT,\n",
    "    Top_Positive_Review_Cust_Date DATE,\n",
    "    Top_Positive_Review_Cust_Comment TEXT,\n",
    "    Top_Positive_Review_Cust_Comment_Title TEXT,\n",
    "    Top_Positive_Review_Cust_Influenced INTEGER,\n",
    "    Top_Positive_Review_Cust_Star_Rating NUMERIC,\n",
    "    Critical_Review_Cust_ID TEXT,\n",
    "    Critical_Review_Cust_Name TEXT,\n",
    "    Critical_Review_Cust_Date DATE,\n",
    "    Critical_Review_Cust_Comment TEXT,\n",
    "    Critical_Review_Cust_Comment_Title TEXT,\n",
    "    Critical_Review_Cust_Influenced INTEGER,\n",
    "    Critical_Review_Cust_Star_Rating NUMERIC,\n",
    "    \"\"\" + \",\\n    \".join([f\"Customer_{i}_ID TEXT, Customer_{i}_Star_Rating NUMERIC, Customer_{i}_Comment TEXT, Customer_{i}_buying_influence INTEGER, Review_Cust_Date_{i} DATE, Review_Cust_Location_{i} TEXT\" for i in range(1, 6)]) + \"\"\"\n",
    "\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "def replace_with_unavailable(value):\n",
    "    if value in [np.nan, 'NaN', 'nan', 'None', 'none', 'N/A', 'n/a', 'NA', 'na', 'null', '']:\n",
    "        return 'Unavailable'\n",
    "    return value\n",
    "\n",
    "def format_date(date_value):\n",
    "    # Use isinstance check to see if date_value is already a float (like NaN)\n",
    "    if isinstance(date_value, (float, int)):\n",
    "        return '0001-01-01'\n",
    "    if isinstance(date_value, str) and re.match(r'\\d{4}-\\d{2}-\\d{2}', date_value):\n",
    "        return date_value\n",
    "    else:\n",
    "        return '0001-01-01'  # Default value for invalid date formats or non-string values\n",
    "\n",
    "\n",
    "def clean_format_data(row):\n",
    "    # Extract values directly, as they are already cleaned\n",
    "    ratings = row['ratings']\n",
    "    price = row['price']\n",
    "    reviews = row['reviews']\n",
    "    product_id = row['Product_ID']\n",
    "    product = row['product']\n",
    "    category = row['category']\n",
    "    url = row['url']\n",
    "\n",
    "    # Use the replace_with_unavailable function for any other columns that might contain null values:\n",
    "    critical_review_id = replace_with_unavailable(row['Critical_Review_Cust_ID'])\n",
    "    critical_review_cust_name = replace_with_unavailable(row['Critical_Review_Cust_Name'])\n",
    "    critical_review_cust_comment = replace_with_unavailable(row['Critical_Review_Cust_Comment'])\n",
    "    critical_review_cust_comment_title = replace_with_unavailable(row['Critical_Review_Cust_Comment_Title'])\n",
    "    critical_review_cust_influenced = replace_with_unavailable(row['Critical_Review_Cust_Influenced'])\n",
    "    critical_review_star_rating = row['Critical_Review_Cust_Star_Rating'] if pd.notna(row['Critical_Review_Cust_Star_Rating']) else 0.0\n",
    "    critical_review_cust_date = row['Critical_Review_Cust_Date'] if row['Critical_Review_Cust_Date'] != 'None' else '0001-01-01'  # Correctly handle NaN values\n",
    "\n",
    "    top_positive_review_id = replace_with_unavailable(row['Top_Positive_Review_Cust_ID'])\n",
    "    top_positive_review_cust_name = replace_with_unavailable(row['Top_Positive_Review_Cust_Name'])\n",
    "    top_positive_review_cust_comment = replace_with_unavailable(row['Critical_Review_Cust_Comment'])\n",
    "    top_positive_review_cust_comment_title = replace_with_unavailable(row['Top_Positive_Review_Cust_Comment_Title'])\n",
    "    top_positive_review_cust_influenced = replace_with_unavailable(row['Top_Positive_Review_Cust_Influenced'])\n",
    "    top_positive_review_star_rating = row['Top_Positive_Review_Cust_Star_Rating'] if pd.notna(row['Top_Positive_Review_Cust_Star_Rating']) else 0.0\n",
    "    top_positive_review_cust_date = row['Top_Positive_Review_Cust_Date'] if row['Top_Positive_Review_Cust_Date'] != 'None' else '0001-01-01'  # Correctly handle NaN values\n",
    "    \n",
    "    top_positive_date = row['Top_Positive_Review_Cust_Date']\n",
    "    critical_review_date = row['Critical_Review_Cust_Date']\n",
    "\n",
    "    def format_date(date_value):\n",
    "        if isinstance(date_value, str) and re.match(r'\\d{4}-\\d{2}-\\d{2}', date_value):\n",
    "            return date_value\n",
    "        else:\n",
    "            return '0001-01-01'  # Default value for invalid date formats or non-string values\n",
    "\n",
    "    top_positive_review_cust_date = format_date(top_positive_date)\n",
    "    critical_review_cust_date = format_date(critical_review_date)\n",
    "\n",
    "\n",
    "\n",
    "    # Handle additional customer information\n",
    "    customer_data = []\n",
    "    for i in range(1, 6):\n",
    "        customer_id = replace_with_unavailable(row[f'Customer_{i}_ID'])\n",
    "        star_rating = row[f'Customer_{i}_Star_Rating'] if pd.notna(row[f'Customer_{i}_Star_Rating']) else 0.0\n",
    "        comment = replace_with_unavailable(row[f'Customer_{i}_Comment'])\n",
    "        buying_influence = row[f'Customer_{i}_buying_influence'] if pd.notna(row[f'Customer_{i}_buying_influence']) else 0\n",
    "        review_cust_date = format_date(row[f'Review_Cust_Date_{i}'])\n",
    "        review_cust_location = replace_with_unavailable(row[f'Review_Cust_Location_{i}'])\n",
    "        customer_data.extend([customer_id, star_rating, comment, buying_influence, review_cust_date, review_cust_location])\n",
    "\n",
    "\n",
    "    # Construct the return tuple\n",
    "    result_tuple = (product_id, product, price, ratings, reviews, category, url, \n",
    "                   top_positive_review_id, top_positive_review_cust_name, top_positive_review_cust_date, \n",
    "                   top_positive_review_cust_comment, top_positive_review_cust_comment_title, \n",
    "                   top_positive_review_cust_influenced, top_positive_review_star_rating, \n",
    "                   critical_review_id, critical_review_cust_name, critical_review_cust_date, critical_review_cust_comment, \n",
    "                   critical_review_cust_comment_title, critical_review_cust_influenced, \n",
    "                   critical_review_star_rating, *customer_data)\n",
    "    \n",
    "    if not result_tuple:\n",
    "        logging.error(f\"Failed to construct tuple for row: {row}\")\n",
    "        return None\n",
    "    \n",
    "    return result_tuple\n",
    "\n",
    "# Check for the presence of the column `Customer_{i}_buying_influence` in the DataFrame\n",
    "for i in range(1, 6):\n",
    "    if f'Customer_{i}_buying_influence' not in df.columns:\n",
    "        logging.error(f\"Column 'Customer_{i}_buying_influence' not found in the DataFrame.\")\n",
    "\n",
    "\n",
    "# Define the INSERT query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO amazon_data_ext (\n",
    "    Product_ID, product, price, ratings, reviews, category, url,\n",
    "    Top_Positive_Review_Cust_ID, Top_Positive_Review_Cust_Name, Top_Positive_Review_Cust_Date, Top_Positive_Review_Cust_Comment, Top_Positive_Review_Cust_Comment_Title, Top_Positive_Review_Cust_Influenced, Top_Positive_Review_Cust_Star_Rating, Critical_Review_Cust_ID, Critical_Review_Cust_Name, Critical_Review_Cust_Date, Critical_Review_Cust_Comment, Critical_Review_Cust_Comment_Title, Critical_Review_Cust_Influenced, Critical_Review_Cust_Star_Rating,\n",
    "    \"\"\" + \", \".join([f\"Customer_{i}_ID, Customer_{i}_Star_Rating, Customer_{i}_Comment, Customer_{i}_buying_influence, Review_Cust_Date_{i}, Review_Cust_Location_{i}\" for i in range(1, 6)]) + \"\"\"\n",
    ") VALUES (\"\"\" + \", \".join([\"%s\"] * (21 + 30)) + \")\"\n",
    "\n",
    "\n",
    "# Count the number of placeholders in the SQL query\n",
    "num_placeholders = insert_query.count('%s')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tuple_values = clean_format_data(row)\n",
    "    if not tuple_values:\n",
    "        logging.warning(f\"Skipping row at index {index} due to errors in data processing.\")\n",
    "        continue\n",
    "    num_tuple_values = len(tuple_values)\n",
    "    \n",
    "    # Check for mismatch between placeholders and tuple values\n",
    "    if num_placeholders != num_tuple_values:\n",
    "        logging.error(f\"Mismatch at index {index}! Number of placeholders: {num_placeholders}, Number of tuple values: {num_tuple_values}\")\n",
    "        logging.error(f\"Tuple values: {tuple_values}\")\n",
    "        \n",
    "        # Expected columns based on the INSERT query\n",
    "        expected_columns = [\n",
    "            \"Product_ID\", \"product\", \"price\", \"ratings\", \"reviews\", \"category\", \"url\",\n",
    "            \"Top_Positive_Review_Cust_ID\", \"Top_Positive_Review_Cust_Name\", \"Top_Positive_Review_Cust_Date\", \"Top_Positive_Review_Cust_Comment\", \"Top_Positive_Review_Cust_Comment_Title\", \"Top_Positive_Review_Cust_Influenced\",\n",
    "            \"Top_Positive_Review_Cust_Star_Rating\", \"Critical_Review_Cust_ID\", \"Critical_Review_Cust_Name\", \"Critical_Review_Cust_Date\", \"Critical_Review_Cust_Comment\", \"Critical_Review_Cust_Comment_Title\",\n",
    "            \"Critical_Review_Cust_Influenced\", \"Critical_Review_Cust_Star_Rating\"\n",
    "        ] + [f\"Customer_{i}_ID\" for i in range(1, 6)] + [f\"Customer_{i}_Star_Rating\" for i in range(1, 6)] + [f\"Customer_{i}_Comment\" for i in range(1, 6)] + [f\"Customer_{i}_buying_influence\" for i in range(1, 6)]\n",
    "\n",
    "\n",
    "\n",
    "        # In the section where you're logging the mismatch error, add this:\n",
    "        for col, val in zip(expected_columns, tuple_values):\n",
    "            print(f\"{col}: {val}\")\n",
    "\n",
    "        \n",
    "        continue  # Skip this iteration\n",
    "\n",
    "\n",
    "    try:\n",
    "        cur.execute(insert_query, tuple_values)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting row at index {index}: {e}\")\n",
    "        logging.debug(f\"Row data: {row}\")\n",
    "        conn.rollback()\n",
    "\n",
    "        \n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "# Rename the columns in the DataFrame\n",
    "df.rename(columns={'ratings': 'star_ratings', 'reviews': 'total_ratings', 'price': 'price_dollars'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file with updated column names\n",
    "df.to_csv('amazon_data_ext.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
